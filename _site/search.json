[
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1:Now You See It!",
    "section": "",
    "text": "In this hands-on exercise,two R packages will be used.They are:\n\ntidyverse\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse,haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "title": "In-class Exercise 1:Now You See It!",
    "section": "",
    "text": "In this hands-on exercise,two R packages will be used.They are:\n\ntidyverse\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse,haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-pisa-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-pisa-data",
    "title": "In-class Exercise 1:Now You See It!",
    "section": "Importing PISA data",
    "text": "Importing PISA data\nThe code chunk below uses ‘read_sas()’ of haven to import PISA data into R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "",
    "text": "According to an office report as shown in the infographic below,\n\nDaily mean temperature are projected to increase by 1.4 to 4.6, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\nAn increase in the intensity of weather variability could present significant challenges to the management of our water resources. Periods of drought can affect the reliability of Singapore’s water supply, while sudden episodes of intense rainfall could overwhelm our drainage system and lead to flash floods.\n\nSource: Ministry of Sustainability and the Environment"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-extraction",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-extraction",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "4.1 Data Extraction",
    "text": "4.1 Data Extraction\nI will first extract the valuable variables. I have selected ‘Year’, ‘Month’, ‘Day’, ‘Daily Rainfall Total (mm)’ as the variables for subsequent analysis. Below are two methods for organizing the data. The second method utilizes the lapply function, which makes data integration more convenient when iterating over multiple files.\n\nMethod 1Method 2\n\n\n\ndata1 &lt;- read_csv(\"data/DAILYDATA_S60_198308.csv\",locale=locale(encoding=\"latin1\"))\ndata1 &lt;- data1 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata2 &lt;- read_csv(\"data/DAILYDATA_S60_199308.csv\",locale=locale(encoding=\"latin1\"))\ndata2 &lt;- data2 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata3 &lt;- read_csv(\"data/DAILYDATA_S60_200308.csv\",locale=locale(encoding=\"latin1\"))\ndata3 &lt;- data3 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata4 &lt;- read_csv(\"data/DAILYDATA_S60_201308.csv\",locale=locale(encoding=\"latin1\"))\ndata4 &lt;- data4 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata5 &lt;- read_csv(\"data/DAILYDATA_S60_202308.csv\",locale=locale(encoding=\"latin1\"))\ndata5 &lt;- data5 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata6 &lt;- read_csv(\"data/DAILYDATA_S60_198312.csv\",locale=locale(encoding=\"latin1\"))\ndata6 &lt;- data6 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata7 &lt;- read_csv(\"data/DAILYDATA_S60_199312.csv\",locale=locale(encoding=\"latin1\"))\ndata7 &lt;- data7 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata8 &lt;- read_csv(\"data/DAILYDATA_S60_200312.csv\",locale=locale(encoding=\"latin1\"))\ndata8 &lt;- data8 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata9 &lt;- read_csv(\"data/DAILYDATA_S60_201312.csv\",locale=locale(encoding=\"latin1\"))\ndata9 &lt;- data9 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\ndata10 &lt;- read_csv(\"data/DAILYDATA_S60_202312.csv\",locale=locale(encoding=\"latin1\"))\ndata10 &lt;- data10 %&gt;% select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n\n\n\n\nfile_names &lt;- c(\"DAILYDATA_S60_198308.csv\", \"DAILYDATA_S60_199308.csv\", \"DAILYDATA_S60_200308.csv\",\"DAILYDATA_S60_201308.csv\", \"DAILYDATA_S60_202308.csv\", \"DAILYDATA_S60_198312.csv\",\"DAILYDATA_S60_199312.csv\", \"DAILYDATA_S60_200312.csv\", \"DAILYDATA_S60_201312.csv\",\"DAILYDATA_S60_202312.csv\")\n\nfile_path_prefix &lt;- \"data/\"\n\ndata_list &lt;- lapply(file_names, function(file_name) {\n  file_path &lt;- paste0(file_path_prefix, file_name) \n  read_csv(file_path, locale = locale(encoding = \"latin1\")) %&gt;%\n    select(Year, Month, Day, `Daily Rainfall Total (mm)`)\n})"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-data-type",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-data-type",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "4.2 Check Data Type",
    "text": "4.2 Check Data Type\nTo ensure that the tables can be merged, we need to make sure that the data type of the ‘Daily Rainfall Total (mm)’ column is the same, specifically numeric type, so we took actions to unify the data type.\n\ndata_frames &lt;- list(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10)\ndata_frames &lt;- lapply(data_frames, function(df) {\n    df$`Daily Rainfall Total (mm)` &lt;- as.numeric(as.character(df$`Daily Rainfall Total (mm)`))\n    return(df)\n})\n\ncombined_data &lt;- bind_rows(data_frames)\n\nwrite_rds(combined_data, \"data/combined_data.rds\")\n\ncombined_data &lt;- read_rds(\"data/combined_data.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#new-column",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#new-column",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "4.3 New column",
    "text": "4.3 New column\nTo make the rainfall amount and its corresponding time more clear in the table for visualization purposes, I will add a ‘Date’ column, facilitating further visual analysis later on.\n\ncombined_data$Date &lt;- as.Date(paste(combined_data$Year, combined_data$Month, combined_data$Day, sep = \"-\"))\nfinal_data &lt;- combined_data[, c('Year','Month','Day','Date','Daily Rainfall Total (mm)' )]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#final-dataset",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#final-dataset",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "4.4 Final Dataset",
    "text": "4.4 Final Dataset\nThe final data presented has 5 variables, 5 columns, and 310 rows. Additionally, it confirms that the years and months are those that I intend to study.\n\nThe Result\n\n\n\nfinal_data\n\n# A tibble: 310 × 5\n    Year Month   Day Date       `Daily Rainfall Total (mm)`\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;                           &lt;dbl&gt;\n 1  1983     8     1 1983-08-01                         0  \n 2  1983     8     2 1983-08-02                         3.7\n 3  1983     8     3 1983-08-03                        60.6\n 4  1983     8     4 1983-08-04                        11.5\n 5  1983     8     5 1983-08-05                         5.8\n 6  1983     8     6 1983-08-06                         0  \n 7  1983     8     7 1983-08-07                         0.9\n 8  1983     8     8 1983-08-08                         0  \n 9  1983     8     9 1983-08-09                         0  \n10  1983     8    10 1983-08-10                         0  \n# ℹ 300 more rows\n\nunique_years &lt;- unique(final_data$Year)\nunique_years\n\n[1] 1983 1993 2003 2013 2023\n\nunique_months &lt;- unique(final_data$Month)\nunique_months\n\n[1]  8 12"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#dry-month",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#dry-month",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "6.1 Dry month",
    "text": "6.1 Dry month\nTo start with, I will filter the data for August from the final_data and calculate the average rainfall for the month of August each year.\n\naugust_data &lt;- filter(final_data, Month == 8)\n\naverage_rainfall_by_year_august &lt;- august_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Average_Rainfall = mean(`Daily Rainfall Total (mm)`, na.rm = TRUE))\n\nFor this visualization, when you hover the mouse over a specific data point, a tooltip will appear displaying the date and the corresponding rainfall for that day. A lightblue line connecting these points represents the average rainfall for August of each year, allowing us to observe whether there is a significant increase in rainfall as the years progress.\n\n\nCode\np &lt;- plot_ly() %&gt;%\n  add_trace(\n    data = august_data,\n    x = ~Year, y = ~`Daily Rainfall Total (mm)`,\n    type = 'box',\n    marker = list(color = '#E2A79A'),\n    boxpoints = 'all',  \n    jitter = 0.3,  \n    pointpos = -1.8 , \n    text = ~paste('Date:', Date, '&lt;br&gt;Rainfall (mm):', `Daily Rainfall Total (mm)`),  \n    hoverinfo = 'text'  \n  ) %&gt;%\n  add_trace(\n    data = average_rainfall_by_year_august,\n    x = ~Year, y = ~Average_Rainfall,\n    type = 'scatter',  \n    mode = 'markers+lines',  \n    marker = list(color = 'lightblue', size = 10),\n    line = list(color = 'lightblue', width = 2)\n  ) %&gt;%\n  layout(\n    title = \"August Rainfall Distribution by Year\",\n    yaxis = list(title = \"Daily Rainfall Total (mm)\", range = c(0, 40)),\n    xaxis = list(title = \"Year\", tickvals = c(1983, 1993, 2003, 2013, 2023)),\n    hovermode = 'closest'\n  )\n\n\np &lt;- p %&gt;% \n  add_trace(\n    data = august_data,\n    x = ~Year, y = ~`Daily Rainfall Total (mm)`,\n    text = ~paste('Date:', Date, '&lt;br&gt;Rainfall (mm):', `Daily Rainfall Total (mm)`),\n    hoverinfo = 'text',\n    type = 'scatter',\n    mode = 'markers',\n    marker = list(color = 'transparent')  \n  )\n\np &lt;- p %&gt;% layout(showlegend = FALSE)\np &lt;- p %&gt;% layout(hoverlabel = list(namelength = 0))\n\np\n\n\n\n\n\n\n\n🔍Insights\nBy connecting the median rainfall of each year with a line (which is lightblue), we can observe a fluctuating trend. In certain years, like 1993 and 2013, the median rainfall seems to have increased, while in other years, such as 1983 and 2023, it appears to have decreased. Overall, there is a slight downward trend, but it’s not particularly pronounced. However, from the boxplot, it can be seen that there are more days without rain."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#wet-month",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#wet-month",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "6.2 Wet month",
    "text": "6.2 Wet month\nFor December, the operation would be the same. I will filter the data for December from the final_data and calculate the average rainfall for the month of December each year.\n\ndecember_data &lt;- filter(final_data, Month == 12)\n\naverage_rainfall_by_year_december &lt;- december_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Average_Rainfall = mean(`Daily Rainfall Total (mm)`, na.rm = TRUE))\n\nAlso in this visualization, when you hover the mouse over a specific data point, a tooltip will appear displaying the date and the corresponding rainfall for that day.\n\n\nCode\np &lt;- plot_ly() %&gt;%\n  add_trace(\n    data = december_data,\n    x = ~Year, y = ~`Daily Rainfall Total (mm)`,\n    type = 'box',\n    marker = list(color = '#A79AE2'),\n    line = list(color = 'orange'),\n    fillcolor = list(color = 'orange',alpha=0.5),\n    boxpoints = 'all',  \n    jitter = 0.3,  \n    pointpos = -1.8 , \n    text = ~paste('Date:', Date, '&lt;br&gt;Rainfall (mm):', `Daily Rainfall Total (mm)`),  # 添加日期信息\n    hoverinfo = 'text' \n  ) %&gt;%\n  add_trace(\n    data = average_rainfall_by_year_december,\n    x = ~Year, y = ~Average_Rainfall,\n    type = 'scatter', \n    mode = 'markers+lines', \n    marker = list(color = '#E2A79A', size = 10),\n    line = list(color = '#E2A79A', width = 2)\n  ) %&gt;%\n  layout(\n    title = \"December Rainfall Distribution by Year\",\n    yaxis = list(title = \"Daily Rainfall Total (mm)\", range = c(0,40)),\n    xaxis = list(title = \"Year\", tickvals = c(1983, 1993, 2003, 2013, 2023)),\n    hovermode = 'closest'\n  )\n\n\np &lt;- p %&gt;% \n  add_trace(\n    data = december_data,\n    x = ~Year, y = ~`Daily Rainfall Total (mm)`,\n    text = ~paste('Date:', Date, '&lt;br&gt;Rainfall (mm):', `Daily Rainfall Total (mm)`),\n    hoverinfo = 'text',\n    type = 'scatter',\n    mode = 'markers',\n    marker = list(color = 'transparent')  \n  )\n\np &lt;- p %&gt;% layout(showlegend = FALSE)\np &lt;- p %&gt;% layout(hoverlabel = list(namelength = 0))\n\np\n\n\n\n\n\n\n\n🔍Insights\nThe daily rainfall in December shows significant fluctuations across different years, with some years (like 1983) having a lower median, while others (like 2023) are higher, but there is no clear consistent upward or downward trend. In 2023, there were extreme rainfall events, with several days having high amounts of rainfall. The variation in the size of the boxplot also reflects the increased overall variability in 2023."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#combine-dry-month-and-wet-month",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#combine-dry-month-and-wet-month",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "6.3 Combine Dry month and Wet month",
    "text": "6.3 Combine Dry month and Wet month\n\n\nCode\np &lt;- plot_ly() %&gt;%\n\n  add_trace(\n    data = august_data,\n    x = ~Year, y = ~`Daily Rainfall Total (mm)`,\n    type = 'box',\n    name = \"August\",\n    marker = list(color = '#E2A79A'),\n    boxpoints = 'all',\n    jitter = 0.3,\n    pointpos = -1.8,\n    text = ~paste('Date:', Date, '&lt;br&gt;Rainfall (mm):', `Daily Rainfall Total (mm)`),\n    hoverinfo = 'text'\n  ) %&gt;%\n\n  add_trace(\n    data = december_data,\n    x = ~Year, y = ~`Daily Rainfall Total (mm)`,\n    type = 'box',\n    name = \"December\",\n    marker = list(color = '#A79AE2'),\n    boxpoints = 'all',\n    jitter = 0.3,\n    pointpos = -1.8,\n    text = ~paste('Date:', Date, '&lt;br&gt;Rainfall (mm):', `Daily Rainfall Total (mm)`),\n    hoverinfo = 'text'\n  ) %&gt;%\n\n  add_trace(\n    data = average_rainfall_by_year_august,\n    x = ~Year, y = ~Average_Rainfall,\n    type = 'scatter',\n    mode = 'markers+lines',\n    name = \"Average Rainfall August\",\n    marker = list(color = 'lightblue', size = 10),\n    line = list(color = 'lightblue', width = 2)\n  ) %&gt;%\n\n  add_trace(\n    data = average_rainfall_by_year_december,\n    x = ~Year, y = ~Average_Rainfall,\n    type = 'scatter',\n    mode = 'markers+lines',\n    name = \"Average Rainfall December\",\n    marker = list(color = '#E2A79A',size = 10),\n    line = list(color = '#E2A79A', width = 2)\n  ) %&gt;%\n\n  layout(\n    title = \"Rainfall Distribution by Year for August and December\",\n    yaxis = list(title = \"Daily Rainfall Total (mm)\", range = c(0, 40)),\n    xaxis = list(title = \"Year\", tickvals = c(1983, 1993, 2003, 2013, 2023)),\n    hovermode = 'closest',\n    showlegend = TRUE\n  ) %&gt;%\n  layout(hoverlabel = list(namelength = 0))\n\np\n\n\n\n\n\n\n\nThis interactive visualization combines the plots from sections 6.1 and 6.2.\nThe light blue box plots and salmon scatter plots represent the precipitation data for August. When hovering over a point, you can see the corresponding date and the total daily precipitation. When hovering over a point on the line graph, you will see the year and the average precipitation for August of that year.\nThe orange box plots and purple scatter plots represent the precipitation data for December. When hovering over a point, you can see the corresponding date and the total daily precipitation. When hovering over a point on the line graph, you will see the year and the average precipitation for December of that year."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#comparison-between-august-and-december",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#comparison-between-august-and-december",
    "title": "Take-home Exercise 3 ：Be Weatherwise or Otherwise",
    "section": "6.4 Comparison between August and December",
    "text": "6.4 Comparison between August and December\n\nIn the decades under examination, there is a clear difference in the rainfall distribution between August and December, and this difference may have increased over time. Therefore, I would like to conduct a preliminary comparison using density plots to see if any insights can be uncovered.\n\n\nCode\naugust_december_data &lt;- final_data %&gt;%\n  filter(Month %in% c(8, 12)) %&gt;%\n  mutate(Month = factor(Month, levels = c(8, 12), labels = c(\"August\", \"December\")),\n         Year = as.factor(Year))  \n\np &lt;- ggplot(august_december_data, aes(x = `Daily Rainfall Total (mm)`, y = Year, fill = Month)) +\n  geom_density_ridges() +\n  scale_fill_manual(values = c(\"August\" = \"#F0776F\", \"December\" = \"#B7E585\")) +\n  labs(title = \"Comparison of Rainfall Distribution between August and December\",\n       x = \"Rainfall (mm)\",\n       y = \"Year\") +\n  theme_ridges() +\n  theme(legend.position = \"right\")\n\np\n\n\n\n\n\n\n🔍Insights\nTrend Over the Years: In the earlier years (such as 1983 and 1993), the density distribution peaks of rainfall in August and December were relatively close; however, in later years (like 2013 and 2023), the difference between the two months seems to be more pronounced.\nExtreme Rainfall Events: Particularly in 2023, the rainfall density curve for December shows a high peak and a long tail, possibly indicating a higher occurrence of extreme rainfall events during this month."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "About Me\n\nStill exploring all the possibilities of the future, but the most formidable task at the moment is learning data analysis and coding.\nAdditionally, the paw prints on this website all originate from my little dog, “Diamond.”\n\n\n\n\n\n\n\nLearning Objectives\n\nUnderstand the basic concepts, theories and methodologies of visual analytics.\nAnalyse data using appropriate visual thinking and visual analytics techniques.\nPresent data using appropriate visual communication and graphical methods.\nDesign and implement cutting-edge visual analytics system for supporting decision making.\n\n\n\n\n\n\nDon’t worry,be happy,have a cup of coffee~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html",
    "title": "Hands-on Exercise 3b：Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "When creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#getting-started",
    "title": "Hands-on Exercise 3b：Programming Animated Statistical Graphics with R",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading the R packages\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\nImporting the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"/Users/zhangshujie/Desktop/SMU/Term 2/Visual/suzyzsj/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex03b/data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"/Users/zhangshujie/Desktop/SMU/Term 2/Visual/suzyzsj/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex03b/data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_at()across() are used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3b：Programming Animated Statistical Graphics with R",
    "section": "Animated Data Visualisation: gganimate methods",
    "text": "Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\nBuilding a static population bubble plot\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\nBuilding the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3b：Programming Animated Statistical Graphics with R",
    "section": "Animated Data Visualisation: plotly",
    "text": "Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\nBuilding an animated bubble plot: ggplotly() method\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position=‘none’) should be used as shown in the plot and code chunk below.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\n\n\nBuilding an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#reference",
    "title": "Hands-on Exercise 3b：Programming Animated Statistical Graphics with R",
    "section": "Reference",
    "text": "Reference\n\nGetting Started\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 6:Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "pacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6:Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "pacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 6:Visualising and Analysing Time-oriented Data",
    "section": "Plotting Calendar Heatmap",
    "text": "Plotting Calendar Heatmap\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\nThe Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\nImporting the data\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\nExamining the data structure\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\nData Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\nBuilding the Calendar Heatmaps\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\nPlotting Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\nPlotting Multiple Calendar Heatmaps\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 6:Visualising and Analysing Time-oriented Data",
    "section": "Plotting Cycle Plot",
    "text": "Plotting Cycle Plot\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\nStep 1: Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\nStep 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\nStep 4: Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\nStep 5: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\nStep 6: Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "title": "Hands-on Exercise 6:Visualising and Analysing Time-oriented Data",
    "section": "Plotting Slopegraph",
    "text": "Plotting Slopegraph\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\nStep 1: Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\nStep 2: Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#overview",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Importing and Preparing The Data Set",
    "text": "Importing and Preparing The Data Set\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\nImporting the data set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\nPreparing the data\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\nTransforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#static-heatmap",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Static Heatmap",
    "text": "Static Heatmap\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\n\nheatmap() of R Stats\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Creating Interactive Heatmap",
    "text": "Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\nWorking with heatmaply\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\nData trasformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\nScaling method\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\nNormalising method\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\nPercentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\nClustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\nManual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\nStatistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\nSeriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\nWorking with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\nThe finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5a:Creating Ternary Plot with R",
    "section": "",
    "text": "For this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5a:Creating Ternary Plot with R",
    "section": "",
    "text": "For this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#data-preparation",
    "title": "Hands-on Exercise 5a:Creating Ternary Plot with R",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 5a:Creating Ternary Plot with R",
    "section": "",
    "text": "Use ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "For this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "For this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#data-preparation",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "Plotting Static Parallel Coordinates Plot",
    "text": "Plotting Static Parallel Coordinates Plot\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\nPlotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\nPlotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\nRotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\nAdjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\nThe basic plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\nRotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\nChanging the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\nParallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#references",
    "title": "Hands-on Exercise 5d: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "References",
    "text": "References\n\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4a:Visualising Distribution",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\npackage ‘distributional’ successfully unpacked and MD5 sums checked package ‘quadprog’ successfully unpacked and MD5 sums checked package ‘ggdist’ successfully unpacked and MD5 sums checked\nThe downloaded binary packages are in C:3aB_packages package ‘ggridges’ successfully unpacked and MD5 sums checked\nThe downloaded binary packages are in C:3aB_packages\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "title": "Hands-on Exercise 4a:Visualising Distribution",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\npackage ‘distributional’ successfully unpacked and MD5 sums checked package ‘quadprog’ successfully unpacked and MD5 sums checked package ‘ggdist’ successfully unpacked and MD5 sums checked\nThe downloaded binary packages are in C:3aB_packages package ‘ggridges’ successfully unpacked and MD5 sums checked\nThe downloaded binary packages are in C:3aB_packages\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 4a:Visualising Distribution",
    "section": "Visualising Distribution with Ridgeline Plot",
    "text": "Visualising Distribution with Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\n\n\n\n\nNote\n\n\n\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n\n\nPlotting ridgeline graph: ggridges method\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\nVarying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Score\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\nMapping the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4a:Visualising Distribution",
    "section": "Visualising Distribution with Raincloud Plot",
    "text": "Visualising Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\nPlotting a Half Eye graph\nFirst, we plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n\n\nAdding the boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\nAdding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\nFinishing touch\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 4b:Visual Statistical Analysis",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Exercise 4b:Visual Statistical Analysis",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#getting-started",
    "title": "Hands-on Exercise 4b:Visual Statistical Analysis",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and launching R packages\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\nImporting data\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\nexam\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\n\nOne-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nUnpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n10.3.5 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n10.3.6 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nOneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\nSignificant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\nSignificant Test of Association (Depedence) : ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-models",
    "title": "Hands-on Exercise 4b:Visual Statistical Analysis",
    "section": "Visualising Models",
    "text": "Visualising Models\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#getting-started-1",
    "title": "Hands-on Exercise 4b:Visual Statistical Analysis",
    "section": "Getting Started",
    "text": "Getting Started"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4b:Visual Statistical Analysis",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\n\npacman::p_load(readxl, performance, parameters, see)\n\n\nImporting Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\nMultiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\nModel Diagnostic: checking for multicolinearity:\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\nModel Diagnostic: checking normality assumption\nIn the code chunk, check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\nModel Diagnostic: Check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\nModel Diagnostic: Complete check\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\nVisualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\nVisualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 4c:Visualising Uncertainty",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#getting-started",
    "title": "Hands-on Exercise 4c:Visualising Uncertainty",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4c:Visualising Uncertainty",
    "section": "Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\nIn this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the mathematical explanation, please refer to Slide 20 of Lesson 4.\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nThe tableThe Code\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\nPlotting standard error bars of point estimates\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates with interactive error bars\nIn this section, we will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nThe plotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 4c:Visualising Uncertainty",
    "section": "Visualising Uncertainty: ggdist package",
    "text": "Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi,\n  show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4c:Visualising Uncertainty",
    "section": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nStep 1: Installing ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nStep 2: Launch the application in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "title": "Hands-on Exercise 4c:Visualising Uncertainty",
    "section": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html",
    "title": "Hands-on Exercise 4d:Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "In this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 4d:Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "In this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#importing-data",
    "title": "Hands-on Exercise 4d:Funnel Plots for Fair Comparisons",
    "section": "Importing Data",
    "text": "Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\ncovid19\n\n# A tibble: 267 × 7\n   `Sub-district ID` City       District `Sub-district` Positive Recovered Death\n               &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;    &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1        3172051003 JAKARTA U… PADEMAN… ANCOL              1776      1691    26\n 2        3173041007 JAKARTA B… TAMBORA  ANGKE              1783      1720    29\n 3        3175041005 JAKARTA T… KRAMAT … BALE KAMBANG       2049      1964    31\n 4        3175031003 JAKARTA T… JATINEG… BALI MESTER         827       797    13\n 5        3175101006 JAKARTA T… CIPAYUNG BAMBU APUS         2866      2792    27\n 6        3174031002 JAKARTA S… MAMPANG… BANGKA             1828      1757    26\n 7        3175051002 JAKARTA T… PASAR R… BARU               2541      2433    37\n 8        3175041004 JAKARTA T… KRAMAT … BATU AMPAR         3608      3445    68\n 9        3171071002 JAKARTA P… TANAH A… BENDUNGAN HIL…     2012      1937    38\n10        3175031002 JAKARTA T… JATINEG… BIDARA CINA        2900      2773    52\n# ℹ 257 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4d:Funnel Plots for Fair Comparisons",
    "section": "FunnelPlotR methods",
    "text": "FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\nFunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\nFunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis.\n\n\nFunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4d:Funnel Plots for Fair Comparisons",
    "section": "Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\nComputing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\nCalculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\nPlotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\nInteractive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2：Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, ggthemes, hrbrthemes,tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00  \n\n\n\n\n\n\n\n\nA comparison between annotations using geom_label(), geom_text() and package ggrepel.\n\nWithoutWith ggrepel\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label_repel(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nWithoutWith ggtheme\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\nggtitle(\"Distribution of Maths scores\") +\ntheme_economist()\n\n\n\n\n\n\n\n\n\n\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nWithout hrbthems plotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18.\nbase_size argument is used to increase the default axis label to 15.\ngrid argument is used to remove the x-axis grid lines.\n\n\nWith hrbthems plotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\np1 plotp1 code\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\np2 plotp2 code\n\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\n\n\n\n\np3 plotp3 code\n\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\nPatchwork package：\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\npatchwork also provides auto-tagging capabilities.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\ninset_element()\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\nFigure below is created by combining patchwork and theme_economist() of ggthemes package.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 2：Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, ggthemes, hrbrthemes,tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data",
    "title": "Hands-on Exercise 2：Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview-of-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview-of-the-data",
    "title": "Hands-on Exercise 2：Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "summary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2：Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "A comparison between annotations using geom_label(), geom_text() and package ggrepel.\n\nWithoutWith ggrepel\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label_repel(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2：Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "ggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nWithoutWith ggtheme\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\nggtitle(\"Distribution of Maths scores\") +\ntheme_economist()\n\n\n\n\n\n\n\n\n\n\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nWithout hrbthems plotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18.\nbase_size argument is used to increase the default axis label to 15.\ngrid argument is used to remove the x-axis grid lines.\n\n\nWith hrbthems plotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 2：Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "p1 plotp1 code\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\np2 plotp2 code\n\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\n\n\n\n\np3 plotp3 code\n\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\nPatchwork package：\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\npatchwork also provides auto-tagging capabilities.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\ninset_element()\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\nFigure below is created by combining patchwork and theme_economist() of ggthemes package.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html",
    "title": "Hands-on Exercise 5e:Treemap Visualisation with R",
    "section": "",
    "text": "pacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5e:Treemap Visualisation with R",
    "section": "",
    "text": "pacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#data-wrangling",
    "title": "Hands-on Exercise 5e:Treemap Visualisation with R",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\nImporting the data set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\nData Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\nGrouped summaries without the Pipe\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\nGrouped summaries with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\n\n\n\n\n\nNote\n\n\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 5e:Treemap Visualisation with R",
    "section": "Designing Treemap with treemap Package",
    "text": "Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\nDesigning a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\nUsing the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\nWorking with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\nColours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\nThe “value” type treemap\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\nThe “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nTreemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\nWorking with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nUsing sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 5e:Treemap Visualisation with R",
    "section": "Designing Treemap using treemapify Package",
    "text": "Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\nDesigning a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\nDefining hierarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 5e:Treemap Visualisation with R",
    "section": "Designing Interactive Treemap using d3treeR",
    "text": "Designing Interactive Treemap using d3treeR\n\nInstalling d3treeR package\n\noptions(repos = c(CRAN = \"https://cran.smu.edu.sg\"))\ninstall.packages(\"devtools\")\n\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\nlibrary(d3treeR)\n\n\n\nDesigning An Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "Importing and Preparing The Data Set",
    "text": "Importing and Preparing The Data Set\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\nImporting Data\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "Building Correlation Matrix: pairs() method",
    "text": "Building Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\nBuilding a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\nDrawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\nIncluding with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "Visualising Correlation Matrix: ggcormat()",
    "text": "Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\n\nThe basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#building-multiple-plots",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "Building multiple plots",
    "text": "Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "Visualising Correlation Matrix using corrplot Package",
    "text": "Visualising Correlation Matrix using corrplot Package\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\nGetting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\nWorking with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\nWorking with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\nWorking with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\nCombining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\nReorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\nReordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#r-packages",
    "title": "Hands-on Exercise 5b:Visual Correlation Analysis",
    "section": "R packages",
    "text": "R packages\n\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html",
    "title": "Hands-on Exercise 3a：Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "First, write a code chunk to check, install and launch the following R packages:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\nThe code chunk below will be used to accomplish the task.\n\npacman::p_load(ggiraph, plotly, patchwork, DT, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 3a：Programming Interactive Data Visualisation with R",
    "section": "Interactive Data Visualisation - ggiraph methods",
    "text": "Interactive Data Visualisation - ggiraph methods\nggiraph  is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\nTooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity",
    "title": "Hands-on Exercise 3a：Programming Interactive Data Visualisation with R",
    "section": "Interactivity",
    "text": "Interactivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n\n\n\n\n\nDisplaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity-1",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity-1",
    "title": "Hands-on Exercise 3a：Programming Interactive Data Visualisation with R",
    "section": "Interactivity",
    "text": "Interactivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n\n\n\n\n\n3.6.1 Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)    \n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\nDisplaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, linewidth = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\nHover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\nStyling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\nCombining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\nCombining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\nClick effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\nCoordinated Multiple Views with ggiraph\nCoordinated multiple views methods has been implemented in the data visualisation below.\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Exercise 3a：Programming Interactive Data Visualisation with R",
    "section": "Interactive Data Visualisation - plotly methods!",
    "text": "Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\nCreating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\nWorking with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\nInteractive:\nClick on the colour symbol at the legend.\n\n\n\n\n\nCreating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\n\n\nCoordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\nThing to learn from the code chunk:\nhighlight_key() simply creates an object of class crosstalk::SharedData. Visit this link to learn more about crosstalk,"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3a：Programming Interactive Data Visualisation with R",
    "section": "Interactive Data Visualisation - crosstalk methods!",
    "text": "Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\nInteractive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\nLinked brushing: crosstalk method\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)   \n\nThings to learn from the code chunk:\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#reference",
    "title": "Hands-on Exercise 3a：Programming Interactive Data Visualisation with R",
    "section": "Reference",
    "text": "Reference\n\nggiraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\nplotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to chheck if tidyverse packages are installed in the computer.If they are, then they will be launched into R.\n\npacman::p_load(tidyverse,patchwork)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to chheck if tidyverse packages are installed in the computer.If they are, then they will be launched into R.\n\npacman::p_load(tidyverse,patchwork)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "R Graphics VS ggplot",
    "text": "R Graphics VS ggplot\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nFigure below shows the seven grammars of ggplot2.\n\nA short description of each building block are as follows:\n\nData: The dataset being plotted.\nAesthetics take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line.\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics, statiscal transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Geometric Objects: geom_bar",
    "text": "Geometric Objects: geom_bar\n\nggplot(data=exam_data, \n       aes(x= MATHS))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Geometric Objects: geom_dotplot",
    "text": "Geometric Objects: geom_dotplot\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\n\n\n\nThe code chunk below performs the following two steps: - scale_y_continuous() is used to turn off the y-axis, and - binwidth argument is used to change the binwidth to 2.5.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_histogram",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_histogram",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Geometric Objects: geom_histogram()",
    "text": "Geometric Objects: geom_histogram()\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()       \n\n\n\n\nNote that the default bin is 30."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Modifying a geometric object by changing geom()",
    "text": "Modifying a geometric object by changing geom()\nIn the code chunk below,\n\nbins argument is used to change the number of bins to 20\nfill argument is used to shade the histogram with color\ncolor argument is used to change the outline colour of the bars\ngeom_vline is used to add mean and median lines\nannotate is used to include descriptions for each line\n\n\nggplot(data=exam_data,\n       aes(x=MATHS)) + \n    geom_histogram(bins=20, \n                   color=\"grey30\", \n                   fill=\"#E1C8C4\") +\n    labs(x = \"Math Score\",\n         y = \"No. of Pupils\",\n         title = \"Distribution of Math Scores\") + \n    geom_vline(aes(xintercept=median(exam_data$MATHS, na.rm=T)),\n               color=\"#595DE5\", linewidth=1, linetype=\"dashed\") + \n    # Add line annotations\n    annotate(\n      \"text\", \n      x = 82, \n      y = 50,\n      label = paste(\"Median =\", round(median(exam_data$MATHS, na.rm=T), 3)),\n      color = \"#595DE5\",\n      size = 3.5\n    ) +\n    geom_vline(aes(xintercept=mean(exam_data$MATHS, na.rm=T)),\n               colour=\"red\", linewidth=1, linetype=\"dashed\") + \n      # Add line annotations\n    annotate(\n      \"text\", \n      x = 60, \n      y = 42,\n      label = paste(\"Mean =\", round(mean(exam_data$MATHS, na.rm=T), 3)),\n      color = \"red\",\n      size = 3.5\n    ) +\n    theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))      \n\n\n\n\n\nChanged no. of binsUsing binwidthBin alignment using boundary & center\n\n\n\n# Original\nb_p1 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 #binwidth=5, \n                 color= 'white',\n                 fill='grey') +\n  theme_gray() +\n  theme(panel.background=element_rect(fill='grey96')) +\n  labs(x = \"Math Score\",\n       y = \"No. of Pupils\",\n       title = \"20 bins\") +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n\n# Changed\nb_p2 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(bins=30, \n                 boundary = 100,\n                 #binwidth=5, \n                 color= 'white',\n                 fill='grey') +\n  theme_gray() +\n  theme(panel.background=element_rect(fill='grey96')) +\n  labs(x = \"Math Score\",\n       y = \"No. of Pupils\",\n       title = \"30 bins\") +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n\n# format plot layout\nb_p1 + b_p2\n\n\n\n\n\n\n\n# Original\nbw_p1 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 #binwidth=5, \n                 color= 'white',\n                 fill='grey') +\n  theme_gray() +\n  theme(panel.background=element_rect(fill='grey96')) +\n  labs(x = \"Math Score\",\n       y = \"No. of Pupils\",\n       title = \"20 bins\") +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))  \n\n# Changed\nbw_p2 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(#bins=20, \n                 boundary = 100,\n                 binwidth=5, \n                 color= 'white',\n                 fill='grey') +\n  theme_gray() +\n  theme(panel.background=element_rect(fill='grey96')) +\n  labs(x = \"Math Score\",\n       y = \"No. of Pupils\",\n       title = \"Binwidth 5 = 19 bins\") +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n\n# format plot layout\nbw_p1 + bw_p2\n\n\n\n\n\n\n\n# Original\nbb_p1 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 #binwidth=5, \n                 color= 'white',\n                 fill='grey') +\n  theme_gray() +\n  theme(panel.background=element_rect(fill='grey96')) +\n  labs(x = \"Math Score\",\n       y = \"No. of Pupils\",\n       title = \"Bin alignment using boundary\") +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n  \n# Changed\nbb_p2 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(bins=20, \n                 center = 50,\n                 #binwidth=5, \n                 color= 'white',\n                 fill='grey') +\n  theme_gray() +\n  theme(panel.background=element_rect(fill='grey96')) +\n  labs(x = \"Math Score\",\n       y = \"No. of Pupils\",\n       title = \"Bin alignment using center\") +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n\n# format plot layout\nbb_p1 + bb_p2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Modifying a geometric object by changing aes()",
    "text": "Modifying a geometric object by changing aes()\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Geometric Objects: geom-density()",
    "text": "Geometric Objects: geom-density()\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Geometric Objects: geom_boxplot()",
    "text": "Geometric Objects: geom_boxplot()\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()            \n\n\n\n\nThe code chunk below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Geometric Objects: geom_violin",
    "text": "Geometric Objects: geom_violin\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Geometric Objects: geom_point()",
    "text": "Geometric Objects: geom_point()\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "geom objects can be combined",
    "text": "geom objects can be combined\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Working with stat()",
    "text": "Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-stat_summary-method",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-stat_summary-method",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Working with stat - the stat_summary() method",
    "text": "Working with stat - the stat_summary() method\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               linewidth=4)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-geom-method",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-geom-method",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Working with stat - the geom() method",
    "text": "Working with stat - the geom() method\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"red\",          \n             linewidth=4)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#adding-a-best-fit-curve-on-a-scatterplot---geom_smooth",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#adding-a-best-fit-curve-on-a-scatterplot---geom_smooth",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Adding a best fit curve on a scatterplot - geom_smooth()",
    "text": "Adding a best fit curve on a scatterplot - geom_smooth()\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)                               \n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_wrap",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_wrap",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Working with facet_wrap()",
    "text": "Working with facet_wrap()\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facet_grid-function",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facet_grid-function",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "facet_grid() function",
    "text": "facet_grid() function\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-coordinate",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-coordinate",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Working with Coordinate",
    "text": "Working with Coordinate\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()          \n\n\n\n\ncoord_flip()\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#changing-the-y--and-x-axis-range",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#changing-the-y--and-x-axis-range",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Changing the y- and x-axis range",
    "text": "Changing the y- and x-axis range\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, linewidth=0.5)  \n\n\n\n\nThe code below fixed both the y-axis and x-axis range from 0-100.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-theme",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-theme",
    "title": "Hands-on Exercise 1：Introduction to Visual Analytics",
    "section": "Working with theme",
    "text": "Working with theme\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications.\nIn this website,you will find my coursework prepared for this course."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "While Singapore has made strides in reducing disparities in family wealth, its education policy is committed to ensuring high-quality education in every school. However, the general public still perceives persistent educational inequalities, especially between elite and ordinary community schools, as well as among students from different socioeconomic backgrounds and immigrant families. We will delve deeply into the state of education in Singapore, based on the data provided by the 2022 Programme for International Student Assessment (PISA)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-and-launching-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-and-launching-r-packages",
    "title": "Take-home Exercise 1",
    "section": "3.1 Install and launching R packages",
    "text": "3.1 Install and launching R packages\nWe use the pacman::p_load() function to load the required R packages into our working environment. The loaded packages are:\n\ntidyverse: A collection of packages for data manipulation and visualization, including ggplot2, dplyr, tidyr, etc.\nggplot2: For creating complex charts and visualizations.\ndplyr: For data manipulation and cleaning.\nreadr: For reading CSV files.\nggpubr: Provides easy-to-use functions for creating publication-quality charts.\nscales: For beautifying charts, like adjusting axes.\nlubridate: For handling date data, if your data includes time information.\nforcats: For handling categorical variables, which could be useful for dealing with variables like gender and school IDs.\ngridExtra or cowplot: For arranging multiple plots on a single page.\ncorrplot: For visualizing correlations between variables.\nHmisc: Contains many functions useful for data analysis, such as descriptive statistics.\nkableExtra:using for creating and beautifying tables. With the kableExtra package, you can easily add titles, footnotes, change table styles, merge cells, and more.\n\n\npacman::p_load(tidyverse, ggplot2, dplyr, readr, ggpubr,\nscales,forcats, gridExtra, corrplot, Hmisc, lubridate,knitr, kableExtra,patchwork)\n\n\n\n\n\nNotes\n\n\nThe R package intsvy allows R users to analyse PISA data among other international large-scale assessments. The use of PISA data via R requires data preparation, and intsvy offers a data transfer function to import data available in other formats directly into R. Intsvy also provides a merge function to merge the student, school, parent, teacher and cognitive databases.\nBut here we only consider the situation in Singapore and don’t need a comparison between countries, so we don’t use this package.\nPlease refer to the details for more information.Instructions"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#dataset-organization",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#dataset-organization",
    "title": "Take-home Exercise 1",
    "section": "3.2 Dataset Organization",
    "text": "3.2 Dataset Organization\nRead ’stu_qqq_SG’file.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\n\n3.2.1 Research Variable Selection\nExtracting valuable variables from the original dataset because we are investigating the distribution of academic performance among Singaporean students in mathematics, reading, and science, as well as the relationship between these performances and students’ schools, gender, and socio-economic status.\nI have extracted the following variables:\n\nCNTSTUID:Intl. Student ID\nCNTSCHID:Intl. School ID\nST004D01T:Student (Standardized) Gender,1 female,2male\nESCS:Index of economic, social and cultural status\nPV1-10MATH:Plausible Value 1-10 in Mathematics\nPV1-10READ:Plausible Value 1-10 in Reading\nPV1-10SCIE:Plausible Value 1-10 in Science\n\nThe previous table information is too cumbersome. I will extract all these variables to create a new table, making it more conducive to subsequent analysis.\n\nstu_qqq_SG_01 &lt;- stu_qqq_SG[, c(\"CNTSTUID\", \"CNTSCHID\", \"ST004D01T\", \"ESCS\",\n                             \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \n                             \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\", \n                             \"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \n                             \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\", \n                             \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \n                             \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")]\n\nwrite_rds(stu_qqq_SG_01,\n          \"data/stu_qqq_SG_01.rds\")\n\n\n\n3.2.2 Data overview\nReviewing the table to obtain an overview and understand the general situation of the data.\n\nstu_qqq_SG_01 &lt;- read_rds(\"data/stu_qqq_SG_01.rds\")\n\n\nFirst 5 RowsDataset StructureData Health\n\n\nDisplaying first 5 rows using head():\n\nhead(stu_qqq_SG_01,5) %&gt;%\n  kbl() %&gt;%\n  kable_material() %&gt;%\n  scroll_box(width = \"400%\", height = \"300px\")\n\n\n\n\n\nCNTSTUID\nCNTSCHID\nST004D01T\nESCS\nPV1MATH\nPV2MATH\nPV3MATH\nPV4MATH\nPV5MATH\nPV6MATH\nPV7MATH\nPV8MATH\nPV9MATH\nPV10MATH\nPV1READ\nPV2READ\nPV3READ\nPV4READ\nPV5READ\nPV6READ\nPV7READ\nPV8READ\nPV9READ\nPV10READ\nPV1SCIE\nPV2SCIE\nPV3SCIE\nPV4SCIE\nPV5SCIE\nPV6SCIE\nPV7SCIE\nPV8SCIE\nPV9SCIE\nPV10SCIE\n\n\n\n\n70200001\n70200052\n1\n0.1836\n639.004\n601.251\n621.480\n631.596\n579.276\n591.791\n600.709\n587.322\n618.131\n581.973\n676.298\n692.247\n690.981\n643.067\n627.908\n684.676\n661.380\n674.070\n666.282\n657.387\n710.634\n618.739\n591.623\n659.770\n635.892\n646.901\n603.569\n621.352\n659.674\n649.719\n\n\n70200002\n70200134\n2\n0.8261\n697.191\n754.277\n671.940\n657.300\n621.126\n655.729\n747.934\n694.365\n742.732\n656.934\n625.585\n686.716\n663.147\n567.435\n614.500\n604.745\n669.375\n623.735\n649.579\n571.261\n670.646\n748.839\n635.443\n639.735\n608.385\n670.662\n734.807\n639.748\n716.768\n655.670\n\n\n70200003\n70200112\n2\n-1.0357\n693.710\n654.450\n696.938\n646.187\n678.119\n644.019\n720.531\n671.425\n694.085\n668.304\n620.116\n559.078\n554.767\n587.026\n591.806\n570.547\n599.078\n545.610\n610.466\n590.758\n666.095\n604.771\n704.217\n687.659\n690.974\n617.175\n692.886\n630.900\n656.620\n649.087\n\n\n70200004\n70200004\n2\n-0.9606\n427.317\n410.376\n423.586\n388.935\n330.962\n379.988\n398.535\n422.127\n375.354\n453.348\n381.495\n400.815\n374.911\n367.484\n336.009\n324.630\n396.242\n374.723\n314.704\n342.956\n340.308\n329.889\n411.353\n327.974\n292.183\n355.423\n400.182\n317.518\n298.893\n362.702\n\n\n70200005\n70200152\n1\n0.0856\n436.462\n453.450\n392.315\n439.986\n443.125\n452.648\n396.970\n459.945\n438.166\n448.084\n448.199\n560.636\n365.478\n469.970\n503.664\n481.215\n436.800\n531.226\n480.997\n478.578\n456.333\n453.400\n498.937\n532.324\n508.231\n504.461\n404.572\n549.457\n411.062\n473.613\n\n\n\n\n\n\n\n\n\nChecking the structure of demo_data using str():\n\nstr(stu_qqq_SG_01)\n\ntibble [6,606 × 34] (S3: tbl_df/tbl/data.frame)\n $ CNTSTUID : num [1:6606] 70200001 70200002 70200003 70200004 70200005 ...\n  ..- attr(*, \"label\")= chr \"Intl. Student ID\"\n $ CNTSCHID : num [1:6606] 70200052 70200134 70200112 70200004 70200152 ...\n  ..- attr(*, \"label\")= chr \"Intl. School ID\"\n $ ST004D01T: num [1:6606] 1 2 2 2 1 1 2 2 1 2 ...\n  ..- attr(*, \"label\")= chr \"Student (Standardized) Gender\"\n $ ESCS     : num [1:6606] 0.1836 0.8261 -1.0357 -0.9606 0.0856 ...\n  ..- attr(*, \"label\")= chr \"Index of economic, social and cultural status\"\n $ PV1MATH  : num [1:6606] 639 697 694 427 436 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Mathematics\"\n $ PV2MATH  : num [1:6606] 601 754 654 410 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Mathematics\"\n $ PV3MATH  : num [1:6606] 621 672 697 424 392 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Mathematics\"\n $ PV4MATH  : num [1:6606] 632 657 646 389 440 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Mathematics\"\n $ PV5MATH  : num [1:6606] 579 621 678 331 443 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Mathematics\"\n $ PV6MATH  : num [1:6606] 592 656 644 380 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Mathematics\"\n $ PV7MATH  : num [1:6606] 601 748 721 399 397 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Mathematics\"\n $ PV8MATH  : num [1:6606] 587 694 671 422 460 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Mathematics\"\n $ PV9MATH  : num [1:6606] 618 743 694 375 438 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Mathematics\"\n $ PV10MATH : num [1:6606] 582 657 668 453 448 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Mathematics\"\n $ PV1READ  : num [1:6606] 676 626 620 381 448 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Reading\"\n $ PV2READ  : num [1:6606] 692 687 559 401 561 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Reading\"\n $ PV3READ  : num [1:6606] 691 663 555 375 365 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Reading\"\n $ PV4READ  : num [1:6606] 643 567 587 367 470 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Reading\"\n $ PV5READ  : num [1:6606] 628 614 592 336 504 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Reading\"\n $ PV6READ  : num [1:6606] 685 605 571 325 481 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Reading\"\n $ PV7READ  : num [1:6606] 661 669 599 396 437 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Reading\"\n $ PV8READ  : num [1:6606] 674 624 546 375 531 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Reading\"\n $ PV9READ  : num [1:6606] 666 650 610 315 481 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Reading\"\n $ PV10READ : num [1:6606] 657 571 591 343 479 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Reading\"\n $ PV1SCIE  : num [1:6606] 711 671 666 340 456 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Science\"\n $ PV2SCIE  : num [1:6606] 619 749 605 330 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Science\"\n $ PV3SCIE  : num [1:6606] 592 635 704 411 499 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Science\"\n $ PV4SCIE  : num [1:6606] 660 640 688 328 532 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Science\"\n $ PV5SCIE  : num [1:6606] 636 608 691 292 508 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Science\"\n $ PV6SCIE  : num [1:6606] 647 671 617 355 504 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Science\"\n $ PV7SCIE  : num [1:6606] 604 735 693 400 405 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Science\"\n $ PV8SCIE  : num [1:6606] 621 640 631 318 549 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Science\"\n $ PV9SCIE  : num [1:6606] 660 717 657 299 411 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Science\"\n $ PV10SCIE : num [1:6606] 650 656 649 363 474 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Science\"\n\n\n\n\nChecking for duplicates:\n\nstu_qqq_SG_01[duplicated(stu_qqq_SG_01),]\n\n# A tibble: 0 × 34\n# ℹ 34 variables: CNTSTUID &lt;dbl&gt;, CNTSCHID &lt;dbl&gt;, ST004D01T &lt;dbl&gt;, ESCS &lt;dbl&gt;,\n#   PV1MATH &lt;dbl&gt;, PV2MATH &lt;dbl&gt;, PV3MATH &lt;dbl&gt;, PV4MATH &lt;dbl&gt;, PV5MATH &lt;dbl&gt;,\n#   PV6MATH &lt;dbl&gt;, PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, PV9MATH &lt;dbl&gt;, PV10MATH &lt;dbl&gt;,\n#   PV1READ &lt;dbl&gt;, PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;, PV5READ &lt;dbl&gt;,\n#   PV6READ &lt;dbl&gt;, PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;, PV10READ &lt;dbl&gt;,\n#   PV1SCIE &lt;dbl&gt;, PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;, PV5SCIE &lt;dbl&gt;,\n#   PV6SCIE &lt;dbl&gt;, PV7SCIE &lt;dbl&gt;, PV8SCIE &lt;dbl&gt;, PV9SCIE &lt;dbl&gt;, …\n\n\nFrom the output, there are no duplicated rows found in stu_qqq_SG_01.\nChecking for missing values:\n\nsum(is.na(stu_qqq_SG_01))\n\n[1] 47\n\n\nFrom the output, there are 47 missing values across all columns in stu_qqq_SG_01.\n\n\n\nWhere is the missing value ??!!\nWe found 47 missing values in the ESCS column.\n\nThe codeThe result\n\n\n\n# Find the location of each missing value in the data set\nmissing_values_locations &lt;- which(is.na(stu_qqq_SG_01), arr.ind = TRUE)\n\n# Display the specific column name\nmissing_values_df &lt;- data.frame(\n  Row = missing_values_locations[, 1],\n  Column = names(stu_qqq_SG_01)[missing_values_locations[, 2]]\n)\n\n# View the location of the missing value\nprint(missing_values_df)\n\n\n\n\n\n    Row Column\n1    73   ESCS\n2   181   ESCS\n3   242   ESCS\n4   442   ESCS\n5   812   ESCS\n6  1152   ESCS\n7  1340   ESCS\n8  1535   ESCS\n9  1580   ESCS\n10 1647   ESCS\n11 1807   ESCS\n12 1926   ESCS\n13 1994   ESCS\n14 2027   ESCS\n15 2046   ESCS\n16 2254   ESCS\n17 2449   ESCS\n18 2463   ESCS\n19 2504   ESCS\n20 3125   ESCS\n21 3134   ESCS\n22 3163   ESCS\n23 3222   ESCS\n24 3327   ESCS\n25 3672   ESCS\n26 3902   ESCS\n27 3931   ESCS\n28 3950   ESCS\n29 4003   ESCS\n30 4154   ESCS\n31 4191   ESCS\n32 4218   ESCS\n33 4673   ESCS\n34 4708   ESCS\n35 4785   ESCS\n36 4898   ESCS\n37 5122   ESCS\n38 5555   ESCS\n39 5560   ESCS\n40 5659   ESCS\n41 5950   ESCS\n42 5990   ESCS\n43 6081   ESCS\n44 6124   ESCS\n45 6157   ESCS\n46 6196   ESCS\n47 6592   ESCS\n\n\n\n\n\nThe Economic, Social, and Cultural Status Index (ESCS) is a crucial indicator, and removing observations with missing ESCS values directly may result in sample bias. I choose to retain the missing values.\n\n\n3.2.3 Processing PV Values\nDue to the presence of 10 plausible values (PVs) for performance in mathematics, reading, and science, I will take the average PV for each subject to facilitate subsequent EDA visualization.\n\nThe resultThe code\n\n\n\n\n# A tibble: 6 × 8\n  CNTSTUID CNTSCHID ST004D01T    ESCS MeanMATH MeanREAD MeanSCIE MeanOverall\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 70200001 70200052         1  0.184      605.     667.     640.        637.\n2 70200002 70200134         2  0.826      690.     628.     672.        663.\n3 70200003 70200112         2 -1.04       677.     583.     660.        640.\n4 70200004 70200004         2 -0.961      401.     361.     344.        369.\n5 70200005 70200152         1  0.0856     436.     476.     479.        464.\n6 70200006 70200043         1  0.127      518.     431.     476.        475.\n\n\n\n\n\nstu_qqq_SG_01 &lt;- stu_qqq_SG_01 %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    MeanMATH = mean(c(PV1MATH, PV2MATH, PV3MATH, PV4MATH, PV5MATH, \n                      PV6MATH, PV7MATH, PV8MATH, PV9MATH, PV10MATH), na.rm = TRUE),\n    MeanREAD = mean(c(PV1READ, PV2READ, PV3READ, PV4READ, PV5READ, \n                      PV6READ, PV7READ, PV8READ, PV9READ, PV10READ), na.rm = TRUE),\n    MeanSCIE = mean(c(PV1SCIE, PV2SCIE, PV3SCIE, PV4SCIE, PV5SCIE, \n                      PV6SCIE, PV7SCIE, PV8SCIE, PV9SCIE, PV10SCIE), na.rm = TRUE),\n    MeanOverall=mean(c(PV1MATH, PV2MATH, PV3MATH, PV4MATH, PV5MATH, \n                      PV6MATH, PV7MATH, PV8MATH, PV9MATH, PV10MATH,\n                      PV1READ, PV2READ, PV3READ, PV4READ, PV5READ, \n                      PV6READ, PV7READ, PV8READ, PV9READ, PV10READ,\n                      PV1SCIE, PV2SCIE, PV3SCIE, PV4SCIE, PV5SCIE, \n                      PV6SCIE, PV7SCIE, PV8SCIE, PV9SCIE, PV10SCIE), na.rm = TRUE)\n  ) %&gt;%\nungroup() %&gt;%\nselect(-matches(\"^PV\\\\d+MATH$\"), -matches(\"^PV\\\\d+READ$\"), \n       -matches(\"^PV\\\\d+SCIE$\"), MeanMATH, MeanREAD, MeanSCIE,MeanOverall)\nhead(stu_qqq_SG_01)\n\n\n\n\nI have now extracted the useful parts from the original table, retaining variables such as CNTSTUID, CNTSCHID, ST004D01T, ESCS, MeanMATH, MeanREAD, MeanSCIE, and MeanOverall. The next step would be to conduct EDA (Exploratory Data Analysis)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-performance-in-math-reading-and-science",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-performance-in-math-reading-and-science",
    "title": "Take-home Exercise 1",
    "section": "4.1 Distribution of performance in math, reading and science",
    "text": "4.1 Distribution of performance in math, reading and science\n\n\nCode\nstu_melted &lt;- reshape2::melt(stu_qqq_SG_01, id.vars = \"CNTSTUID\", measure.vars = c(\"MeanMATH\", \"MeanREAD\", \"MeanSCIE\"))\n\naverage_scores &lt;- stu_melted %&gt;%\n  group_by(variable) %&gt;%\n  summarise(MeanScore = mean(value, na.rm = TRUE))\n\np1 &lt;- ggplot(stu_melted, aes(x = variable, y = value, fill = variable)) +\n  geom_boxplot(outlier.colour = \"blue\", outlier.shape = 1) +\n  geom_text(data = average_scores, aes(label = round(MeanScore, 1), y = MeanScore), \n            position = position_dodge(width = 0.75), vjust =1.8, color = \"black\", size = 3.5) +\n  stat_summary(fun.y=mean, geom=\"point\", shape=20, size=3, color=\"red\", fill=\"red\") +\n  scale_x_discrete(labels = c(\"MeanMATH\" = \"Math\", \"MeanREAD\" = \"Reading\", \"MeanSCIE\" = \"Science\")) +\n  scale_fill_brewer(palette = \"Pastel1\") +\n  labs(title = \"Distribution of subject performance value\",\n       x = \"Subject\",\n       y = \"Performance value\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\np2 &lt;- ggplot(stu_qqq_SG_01, aes(x = MeanOverall)) +\n  geom_point(aes(y = MeanMATH, color = \"Math\"), alpha = 0.5) +\n  geom_smooth(aes(y = MeanMATH, color = \"Math\"), method = 'lm') +\n  geom_point(aes(y = MeanREAD, color = \"Reading\"), alpha = 0.5) +\n  geom_smooth(aes(y = MeanREAD, color = \"Reading\"), method = 'lm') +\n  geom_point(aes(y = MeanSCIE, color = \"Science\"), alpha = 0.5) +\n  geom_smooth(aes(y = MeanSCIE, color = \"Science\"), method = 'lm') +\n  scale_color_manual(values = c(\"Math\" = \"salmon\", \"Reading\" = \"steelblue1\", \"Science\" = \"darkseagreen\"),\n                     name = \"Subject\", \n                     labels = c(\"Math\" = \"Math\", \"Reading\" = \"Reading\", \"Science\" = \"Science\")) +\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  labs(title = \"Relationship Between MeanOverall and Three Subjects\",\n       x = \"MeanOverall\", y = \"Score\")\n\np1+p2\n\n\n\n\n\n\nInsights：\n\nSingaporean students perform best on average in Mathematics, with Reading being the lowest.\nReading and Science seem to have more outliers, suggesting that scores in these subjects may have more extreme cases.\nHowever, overall, the average scores of the three subjects are not significantly different, indicating a relatively balanced performance across subjects among Singaporean students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-relationship-between-gender-and-academic-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-relationship-between-gender-and-academic-performance",
    "title": "Take-home Exercise 1",
    "section": "4.2 The relationship between gender and academic performance",
    "text": "4.2 The relationship between gender and academic performance\n\n\nCode\nstu_long &lt;- reshape2::melt(stu_qqq_SG_01, id.vars = c(\"CNTSTUID\", \"ST004D01T\"), \n                 measure.vars = c(\"MeanMATH\", \"MeanREAD\", \"MeanSCIE\",\"MeanOverall\"),\n                 variable.name = \"Subject\", value.name = \"Score\")\n\nstu_long$Subject &lt;- factor(stu_long$Subject, \n                           levels = c(\"MeanMATH\", \"MeanREAD\", \"MeanSCIE\",\"MeanOverall\"),\n                           labels = c(\"Math\", \"Reading\", \"Science\",\"Overall\"))\n\n# Calculate the average score for each subject and gender combination\naverage_scores &lt;- stu_long %&gt;%\n  group_by(Subject, ST004D01T) %&gt;%\n  summarise(AverageScore = mean(Score, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Plot box plots and add numerical values of average scores, using facet_wrap to arrange into a two-row and two-column layout\nggplot(stu_long, aes(x = as.factor(ST004D01T), y = Score, fill = as.factor(ST004D01T))) +\n  geom_boxplot(outlier.colour = \"red\", outlier.shape = 1) +\n  geom_text(data = average_scores, aes(label = sprintf(\"%.1f\", AverageScore), \n                                       y = AverageScore), \n            position = position_dodge(width = 0.75), \n            size = 2.5, \n            color = \"black\",\n            vjust = 1.5) +\n  facet_wrap(~ Subject, ncol = 2) + \n  labs(x = \"Gender\", y = \"Average Score\", \n       title = \"The Relationship Between Gender and Academic Performance\") +\n  scale_fill_discrete(name = \"Gender\", labels = c(\"1\" = \"Female\", \"2\" = \"Male\")) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nInsights:\n\nIn Mathematics and Science, boys have higher average scores than girls. In Reading, girls have higher average scores than boys.\nAcross all three subjects, the distribution of boys’ scores is more concentrated, suggesting they might have more consistency in their performance. In contrast, the distribution of girls’ scores in Mathematics and Science is more dispersed, indicating a wider range of variability in their scores.\nAlthough the overall academic performance across subjects is balanced between boys and girls, girls are slightly ahead. Girls’ performance across subjects appears to be more stable."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-relationship-between-school-and-academic-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-relationship-between-school-and-academic-performance",
    "title": "Take-home Exercise 1",
    "section": "4.3 The relationship between school and academic performance",
    "text": "4.3 The relationship between school and academic performance\n\n\nCode\nnumber_of_schools &lt;- stu_qqq_SG_01 %&gt;%\n  summarise(NumberOfSchools = n_distinct(CNTSCHID))\n\nnumber_of_schools\n\n\n# A tibble: 1 × 1\n  NumberOfSchools\n            &lt;int&gt;\n1             164\n\n\nThere are a total of 164 schools.\n\n\nCode\nlong_data &lt;- stu_qqq_SG_01 %&gt;%\n  gather(key = \"Subject\", value = \"Score\", MeanMATH, MeanREAD, MeanSCIE, MeanOverall) %&gt;%\n  group_by(CNTSCHID, Subject) %&gt;%\n  summarise(AverageScore = mean(Score, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Label = ifelse(AverageScore &gt; 650 | AverageScore &lt; 450, as.character(CNTSCHID), \"\"))\n\n# Create a dot plot to compare the average performance of different schools\nggplot(long_data, aes(x = CNTSCHID, y = AverageScore, color = Subject)) +\n  geom_point(position = position_dodge(width = 0.5)) +\n  geom_text(aes(label = Label), check_overlap = TRUE, vjust = -1, position = position_dodge(width = 0.5)) +\n  theme_minimal() +\n  labs(title = \"Comparison of School Performance in Subjects\",\n       x = \"School ID\", y = \"Average Score\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) \n\n\n\n\n\nFrom the graph, it is evident that the performance levels of most schools in Singapore are concentrated between 450 and 650, with only a few falling outside this range. I have marked the school IDs that fall outside this range. It can be seen that the school with IDs ending in 01、40、82、101、199、155shows excellent performance in all three subjects, while schools with IDs ending in 115 and 149 exhibit relatively poor performance in all three subjects.\n\n\nCode\nschool_performance &lt;- stu_qqq_SG_01 %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise(\n    AvgMath = mean(MeanMATH, na.rm = TRUE),\n    AvgReading = mean(MeanREAD, na.rm = TRUE),\n    AvgScience = mean(MeanSCIE, na.rm = TRUE),\n    AvgOverall= mean(MeanOverall, na.rm = TRUE)\n  ) %&gt;%\n  gather(key = \"Subject\", value = \"AverageScore\", AvgMath, AvgReading, AvgScience,AvgOverall)\n\n# Use box plots to visualize the average scores of different schools\nggplot(school_performance, aes(x = CNTSCHID, y = AverageScore, fill = Subject)) +\n  geom_boxplot() +\n  facet_wrap(~ Subject, scales = \"fixed\") +\n  theme_minimal() +\n  labs(x = \"School ID\", y = \"Average Score\", title = \"School Influence on Academic Performance\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) \n\n\n\n\n\nFrom the box plots, it is observable that the average academic performance of most schools in Singapore lies between 500 and 600. The majority of schools perform better in Mathematics and Science, with slightly lower performance in Reading, yet still within a reasonable range. The mean values also fall within a sensible range, with very few outliers.\n\n\nCode\n# Calculate the quartiles and outlier thresholds for each course in every school\nschool_quartiles &lt;- school_performance %&gt;%\n  group_by(CNTSCHID, Subject) %&gt;%\n  summarise(\n    Q1 = quantile(AverageScore, 0.25, na.rm = TRUE),\n    Q3 = quantile(AverageScore, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    LowerBound = Q1 - 1.5 * IQR\n  ) %&gt;%\n  ungroup()\n\n# Use inner_join to find outliers in the original data\noutliers &lt;- inner_join(school_performance, school_quartiles, by = c(\"CNTSCHID\", \"Subject\")) %&gt;%\n  filter(AverageScore &lt; LowerBound) %&gt;%\n  arrange(CNTSCHID, Subject)\n\n# Identify school IDs that overlap in the lower outliers for Mathematics, Reading, Science, and Overall Average Scores\noverlapping_schools &lt;- outliers %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  filter(n() &gt; 1) %&gt;%\n  arrange(CNTSCHID) %&gt;%\n  ungroup()\n\n# Examine the overlapping outliers\noverlapping_schools\n\n\n# A tibble: 0 × 7\n# ℹ 7 variables: CNTSCHID &lt;dbl&gt;, Subject &lt;chr&gt;, AverageScore &lt;dbl&gt;, Q1 &lt;dbl&gt;,\n#   Q3 &lt;dbl&gt;, IQR &lt;dbl&gt;, LowerBound &lt;dbl&gt;\n\n\nThe results show that there is no single school with outliers in the same subjects, proving that the outliers originate from different schools.\n\nInsights:\n\nThe overall level of schools in Singapore tends to be balanced, with no significant differences and no “extreme schools” appearing in the high or low outliers for each subject.\nHowever, there is still some differentiation in subject performance levels among schools, as some schools, despite not appearing in the outliers, exhibit overall higher or lower levels in certain subjects.\nAdditionally, the number of schools excelling in subject performance is greater than those performing below the average level.\nMost schools fall within a reasonable range of performance, with both strengths and weaknesses coexisting."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-relationship-between-socioeconomic-status-and-academic-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-relationship-between-socioeconomic-status-and-academic-performance",
    "title": "Take-home Exercise 1",
    "section": "4.4 The relationship between socioeconomic status and academic performance",
    "text": "4.4 The relationship between socioeconomic status and academic performance\nSince the three subjects and the overall average scores show a positive correlation, I have chosen to analyze the relationship between socioeconomic status and academic performance using MeanOverall and MeanSCIE.\n\nScatter plotCorrelation test\n\n\n\n\nCode\nggplot(stu_qqq_SG_01, aes(x = ESCS)) +\n  geom_point(aes(y = MeanOverall, color = \"Overall\"), alpha = 0.5) +\n  geom_smooth(aes(y = MeanOverall, color = \"Overall\"), method = 'lm') +\n  geom_point(aes(y = MeanSCIE, color = \"Science\"), alpha = 0.5) +\n  geom_smooth(aes(y = MeanSCIE, color = \"Science\"), method = 'lm') +\n  scale_color_manual(values = c(\"Overall\" = \"lightseagreen\", \"Science\" = \"palevioletred1\"),\n                     name = \"Subject\", \n                     labels = c(\"Overall\" = \"Mean Overall\", \"Science\" = \"Mean Science\")) +\n  theme_minimal() +\n  labs(title = \"Relationship Between ESCS and Academic Performance\",\n       x = \"ESCS Index\", y = \"Score\")\n\n\n\n\n\n\n\nTo determine if there is an association between the two groups, conduct a correlation analysis test.\n\n\nCode\ncor_overall_escs &lt;- cor(stu_qqq_SG_01$MeanOverall, stu_qqq_SG_01$ESCS, use = \"complete.obs\")\ncor_science_escs &lt;- cor(stu_qqq_SG_01$MeanSCIE, stu_qqq_SG_01$ESCS, use = \"complete.obs\")\n\nlm_overall &lt;- lm(MeanOverall ~ ESCS, data = stu_qqq_SG_01)\nlm_science &lt;- lm(MeanSCIE ~ ESCS, data = stu_qqq_SG_01)\n\nlm_overall\n\n\n\nCall:\nlm(formula = MeanOverall ~ ESCS, data = stu_qqq_SG_01)\n\nCoefficients:\n(Intercept)         ESCS  \n     544.90        51.28  \n\n\nCode\nlm_science\n\n\n\nCall:\nlm(formula = MeanSCIE ~ ESCS, data = stu_qqq_SG_01)\n\nCoefficients:\n(Intercept)         ESCS  \n     547.01        50.03  \n\n\nThere is a significant positive correlation between ESCS and students’ overall average scores and science scores, indicating that a higher economic, social, and cultural status is associated with better academic performance. Students who perform poorly due to unfavorable socioeconomic conditions may require some socioeconomic assistance.\n\n\n\n\nInsights: - Due to the positive correlation observed among the subjects, the scatter plot and correlation analysis indicate that there is a certain relationship between academic performance and socioeconomic status. Students from lower socioeconomic backgrounds tend to have poorer academic performance, while those from better socioeconomic circumstances tend to perform better academically.\n\nBased on these findings, recommendations can be made to Singaporean government departments and educational institutions. It is suggested to provide support and assistance to students from lower socioeconomic backgrounds to explore the possibility of improving overall academic performance. It is also possible that students from higher socioeconomic backgrounds have the opportunity to attend schools with stronger teaching capabilities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "",
    "text": "This take-home exercise is done based on a take-home exercise 1 submission prepared by a classmate. The peer submission will be critiqued in terms of clarity and aesthetics, and the original design will be remade using the data visualization principles and best practice learnt in Lesson 1 and 2."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#installing-r-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#installing-r-packages",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "Installing R Packages",
    "text": "Installing R Packages\nThe code chunk below uses pacman::p_load() to check if packages are installed. If they are, they will be launched into R. The packages installed are\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\nhaven: an R package designed for reading and writing datasets from software like SAS, SPSS, and Stata, into R.\ngridExtra: an R package that extends the grid graphics system.\nggplot2: a widely used R package for creating elegant and informative graphics using the Grammar of Graphics framework.\nplotly: an R package for interactive, web-based charts and dashboards, extending ggplot2 capabilities.\nggridges: an R package extending ggplot2 for concise and effective ridge plots.\n\n\nCode\npacman::p_load(ggrepel, ggthemes, hrbrthemes, patchwork, tidyverse, haven, gridExtra, ggplot2, plotly, ggridges)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#dataset",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#dataset",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "Dataset",
    "text": "Dataset\nOne dataset (Student questionnaire data file) from PISA is provided for the task.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\nFollowing this student’s approach, select the variables related to the project and name them.\n\nVariablesThe Code\n\n\n\n\n\nVariable Code\nVariable Name\n\n\n\n\nCNTSCHID\nSchool_ID\n\n\nCNTSTUID\nStudent_ID\n\n\nST004D01T\nGender\n\n\nPV1MATH\nMath_Score\n\n\nPV1READ\nReading_Score\n\n\nPV1SCIE\nScience_Score\n\n\nESCS\nSocioeconomic_Stat\n\n\nST251Q07JA\nArtwork\n\n\n\n\n\n\nstu_qqq_eda &lt;- stu_qqq_SG %&gt;%\n  select(CNTSCHID,CNTSTUID,ST004D01T, PV1MATH, PV1READ, PV1SCIE, ST250Q01JA, ST250Q03JA, ST251Q07JA, ESCS)%&gt;%\n  rename(c(Gender=\"ST004D01T\", \n           School_ID=\"CNTSCHID\", \n           Student_ID=\"CNTSTUID\",\n           Math_Score=\"PV1MATH\",\n           Reading_Score=\"PV1READ\",\n           Science_Score=\"PV1SCIE\",\n           Own_Room=\"ST250Q01JA\",\n           Edu_App=\"ST250Q03JA\",\n           Artwork=\"ST251Q07JA\",\n           Socioeconomic_Stat=\"ESCS\"))\n\n\n\n\nChanging Data Types\n\n\nCode\nstu_qqq_eda$School_ID = as.factor(stu_qqq_eda$School_ID)\nstu_qqq_eda$Gender = as.factor(stu_qqq_eda$Gender)\n\n\nRecode Variables\n\nGenderArtworkSocioeconomic Status\n\n\n\nstu_qqq_eda &lt;- stu_qqq_eda %&gt;%\n  mutate(Gender=(recode(Gender, '1'=\"Female\", '2'=\"Male\")))\n\n\n\n\nstu_qqq_eda$Artwork &lt;- factor(stu_qqq_eda$Artwork,\n                               levels = c(1,2,3,4),\n                               labels = c(\"None\",\"One\",\"Two\",\"Three+\"))\nstu_data_artwork &lt;- stu_qqq_eda[!is.na(stu_qqq_eda$Artwork), ]\n\n\n\n\nstu_qqq_socio &lt;- stu_qqq_eda %&gt;%\n  select(Math_Score, Science_Score, Reading_Score, Socioeconomic_Stat) %&gt;%\n  drop_na()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-1academic-performance",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-1academic-performance",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "Visualization 1:Academic Performance",
    "text": "Visualization 1:Academic Performance\n\nMathematicsReadingScience\n\n\n\n\nCode\nggplot(data=stu_qqq_eda, \n       aes(x=Math_Score)) +\n  geom_histogram(color= \"grey10\",\n           fill= '#ADD0B3') +\n  labs(x = \"Math Scores\",\n       y =\"Number of Students\",\n       title= \"Distribution of Student Math Scores\") +\n  geom_vline(aes(xintercept=median(stu_qqq_eda$Math_Score, na.rm = T)), \n             color=\"#FFFFFF\", \n             linewidth=1, \n             linetype=\"dashed\") + \n  annotate(\"text\", \n           x=675, \n           y=60, \n          label= paste(\"Median =\", round(median(stu_qqq_eda$Math_Score, na.rm = T), 3)), \n           color= \"white\", \n           size=4,\n           ) +\n  geom_vline(aes(xintercept=mean(stu_qqq_eda$Math_Score, na.rm = T)),\n             colour=\"black\",\n             linewidth=1,\n             linetype=\"dashed\") +\n  annotate(\"text\",\n           x=495,\n           y=50,\n           label=paste(\"Mean =\", round(mean(stu_qqq_eda$Math_Score, na.rm = T), 3)),\n           color=\"black\",\n           size=4) +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=stu_qqq_eda, \n       aes(x=Reading_Score)) +\n  geom_histogram(color= \"grey10\",\n           fill= '#ADD0B3') +\n  labs(x= \"Reading Scores\",\n       y=\"Number of Students\",\n       title= \"Distribution of Student Reading Scores\") +\n  geom_vline(aes(xintercept=median(stu_qqq_eda$Reading_Score, na.rm = T)), \n             color=\"#FFFFFF\", \n             linewidth=1, \n             linetype=\"dashed\") + \n  annotate(\"text\", \n           x=650, \n           y=60, \n           label= paste(\"Median =\", round(median(stu_qqq_eda$Reading_Score, na.rm = T), 3)), \n           color= \"#FFFFFF\", \n           size=4) +\n  geom_vline(aes(xintercept=mean(stu_qqq_eda$Reading_Score, na.rm = T)),\n             colour=\"black\",\n             linewidth=1,\n             linetype=\"dashed\") +\n  annotate(\"text\",\n           x=460,\n           y=50,\n           label=paste(\"Mean =\", round(mean(stu_qqq_eda$Reading_Score, na.rm = T), 3)),\n           color=\"black\",\n           size=4) +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=stu_qqq_eda, \n       aes(x=Science_Score)) +\n  geom_histogram(color= \"grey10\",\n           fill= '#ADD0B3') +\n  labs(x= \"Science Scores\",\n       y=\"Number of Students\",\n       title= \"Distribution of Student Science Scores\") +\n  geom_vline(aes(xintercept=median(stu_qqq_eda$Science_Score, na.rm = T)), \n             color=\"#FFFFFF\", \n             linewidth=1, \n             linetype=\"dashed\") + \n  annotate(\"text\", \n           x=659, \n           y=60, \n           label= paste(\"Median =\", round(median(stu_qqq_eda$Science_Score, na.rm = T), 3)), \n           color= \"#FFFFFF\", \n           size=4) +\n  geom_vline(aes(xintercept=mean(stu_qqq_eda$Science_Score, na.rm = T)),\n             colour=\"black\",\n             linewidth=1,\n             linetype=\"dashed\") +\n  annotate(\"text\",\n           x=480,\n           y=50,\n           label=paste(\"Mean =\", round(mean(stu_qqq_eda$Science_Score, na.rm = T), 3)),\n           color=\"black\",\n           size=4) +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\",colour=\"#f5f5f5\"))\n\n\n\n\n\n\n\n\n\nCritique\n\nClarity：\n\nDifficulty in comparing multiple distributions: If there is a need to compare the distributions of two or more different datasets, a single histogram might not be as clear as a combined chart.\nIndistinct median and outliers: Histograms do not highlight the median and outliers as clearly as box plots. Box plots provide a way to quickly identify median, outliers, and other important statistical information.\nUnclear data range: Histograms may not always clearly show the maximum and minimum values of data, whereas box plots explicitly mark these values.\n\nAesthetics：\n\nLack of intuitive display of data dispersion: Histograms can show the pattern of data distribution, but they do not display the spread of data as intuitively as box plots, including quartiles and outliers.\nLack of space efficiency: A single chart may visually occupy more space because it requires additional charts to provide the information that a box plot can offer.\n\n\n\n\nRemake\n\nMathematicsReadingScience\n\n\n\n\nCode\nmedian_math &lt;- round(median(stu_qqq_eda$Math_Score, na.rm = TRUE), 0)\nmean_math &lt;- round(mean(stu_qqq_eda$Math_Score, na.rm = TRUE), 0)\nmin_math &lt;- round(min(stu_qqq_eda$Math_Score, na.rm = TRUE), 0)\nmax_math &lt;- round(max(stu_qqq_eda$Math_Score, na.rm = TRUE), 0)\n\n# histogram\nhistogram &lt;- ggplot(stu_qqq_eda, aes(x=Math_Score)) +\n  geom_histogram(color=\"grey10\", fill='#ADD0B3', bins=30) +\n  geom_vline(aes(xintercept=median_math), color=\"#FFFFFF\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=675, y=600, label=paste(\"Median =\", median_math), color=\"black\", size=4) +\n  geom_vline(aes(xintercept=mean_math), colour=\"black\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=495, y=600, label=paste(\"Mean =\", mean_math), color=\"black\", size=4) +\n  labs(x = \"Math Scores\", y =\"Number of Students\", title= \"Distribution of Student Math Scores\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"))\n\n# boxplot\nboxplot &lt;- ggplot(stu_qqq_eda, aes(y=Math_Score, x=\"\")) +\n  geom_boxplot(fill='#ADD0B3') +\n  stat_summary(fun=mean, geom=\"point\", shape=23, size=2, color=\"darkred\", fill=\"red\") +\n  geom_text(x=0.5, y=min_math, label=min_math, vjust=-0.5) +\n  geom_text(x=0.5, y=max_math, label=max_math, vjust=-0.5) +\n  coord_flip() +\n  labs(title=\"Boxplot of Math Scores\", x=\"\", y=\"\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# combined plot\ncombined_plot &lt;- boxplot + histogram + plot_layout(heights = c(1, 3))\ncombined_plot\n\n\n\n\n\n\n\n\n\nCode\nmedian_reading &lt;- round(median(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\nmean_reading &lt;- round(mean(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\nmin_reading &lt;- round(min(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\nmax_reading &lt;- round(max(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\n\nhistogram_reading &lt;- ggplot(stu_qqq_eda, aes(x=Reading_Score)) +\n  geom_histogram(color=\"grey10\", fill='#ADD0B3', bins=30) +\n  geom_vline(aes(xintercept=median_reading), color=\"#FFFFFF\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=650, y=600, label=paste(\"Median =\", median_reading), color=\"black\", size=4) +\n  geom_vline(aes(xintercept=mean_reading), colour=\"black\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=460, y=600, label=paste(\"Mean =\", mean_reading), color=\"black\", size=4) +\n  labs(x= \"Reading Scores\", y=\"Number of Students\", title= \"Distribution of Student Reading Scores\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"))\n\nboxplot_reading &lt;- ggplot(stu_qqq_eda, aes(y=Reading_Score, x=\"\")) +\n  geom_boxplot(fill='#ADD0B3') +\n  stat_summary(fun=mean, geom=\"point\", shape=23, size=2, color=\"darkred\", fill=\"red\") +\n  geom_text(x=0.5, y=min_reading, label=min_reading, vjust=-0.5) +\n  geom_text(x=0.5, y=max_reading, label=max_reading, vjust=-0.5) +\n  coord_flip() +\n  labs(title=\"Boxplot of Reading Scores\", x=\"\", y=\"\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\ncombined_plot_reading &lt;- boxplot_reading + histogram_reading + plot_layout(heights = c(1, 3))\ncombined_plot_reading\n\n\n\n\n\n\n\n\n\nCode\nmedian_science &lt;- round(median(stu_qqq_eda$Science_Score, na.rm = TRUE), 0)\nmean_science &lt;- round(mean(stu_qqq_eda$Science_Score, na.rm = TRUE), 0)\nmin_science &lt;- round(min(stu_qqq_eda$Science_Score, na.rm = TRUE), 0)\nmax_science &lt;- round(max(stu_qqq_eda$Science_Score, na.rm = TRUE), 0)\n\nhistogram_science &lt;- ggplot(stu_qqq_eda, aes(x=Science_Score)) +\n  geom_histogram(color=\"grey10\", fill='#ADD0B3', bins=30) +\n  geom_vline(aes(xintercept=median_science), color=\"#FFFFFF\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=659, y=600, label=paste(\"Median =\", median_science), color=\"black\", size=4) +\n  geom_vline(aes(xintercept=mean_science), colour=\"black\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=480, y=600, label=paste(\"Mean =\", mean_science), color=\"black\", size=4) +\n  labs(x= \"Science Scores\", y=\"Number of Students\", title= \"Distribution of Student Science Scores\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"))\n\nboxplot_science &lt;- ggplot(stu_qqq_eda, aes(y=Science_Score, x=\"\")) +\n  geom_boxplot(fill='#ADD0B3') +\n  stat_summary(fun=mean, geom=\"point\", shape=23, size=2, color=\"darkred\", fill=\"red\") +\n  geom_text(x=0.5, y=min_science, label=min_science, vjust=-0.5) +\n  geom_text(x=0.5, y=max_science, label=max_science, vjust=-0.5) +\n  coord_flip() +\n  labs(title=\"Boxplot of Science Scores\", x=\"\", y=\"\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\ncombined_plot_science &lt;- boxplot_science + histogram_science + plot_layout(heights = c(1, 3))\ncombined_plot_science\n\n\n\n\n\n\n\n\n\n\n🔍Improvement\n\nThe combination of histograms and box plots: By using the plot_layout() function, histograms and box plots are combined together, allowing both types of charts to be viewed within the same visualization, facilitating comparison and analysis.\nThe use of the theme_minimal() theme: This unifies the aesthetic style of the charts, making them more streamlined and modern.\nAdjustment of annotations and indicator lines: Through the use of annotate() and geom_vline(), dashed lines and text annotations representing the mean and median have been added to the histogram, making these key statistical indicators immediately clear.\nOptimization of chart layout and proportions: The relative height proportions of the histograms and box plots have been adjusted using the plot_layout() to ensure visual coherence."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-2student-performances-variations-within-schools",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-2student-performances-variations-within-schools",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "Visualization 2:Student Performances Variations Within Schools",
    "text": "Visualization 2:Student Performances Variations Within Schools\nThe below code chunk begins by grouping the data based on the unique school IDs. Then, it calculates the average scores for each subject, creating new variables. Only distinct combinations of school IDs and their corresponding average scores for each subject are included.\n\n\nCode\nstu_sch &lt;- stu_qqq_eda %&gt;%\n  group_by(School_ID) %&gt;%\n  mutate(math_avg = mean(Math_Score),\n         science_avg = mean(Science_Score),\n         reading_avg = mean(Reading_Score)) %&gt;%\n  select(School_ID, math_avg, science_avg, reading_avg) %&gt;%\n  unique() \n\n\nBelow shows the histograms of the different average scores for each subject across different schools. The histogram is customized with 30 bins, with a customized color scheme, and median labels and values. This provides a comprehensive view of how average scores vary within schools for each subject.\n\nMathematicsReadingScience\n\n\n\n\nCode\nsch_math &lt;- ggplot(data = stu_sch,\n       aes(x = math_avg)) +\n  geom_histogram(bins=30,\n                 color = \"grey50\",\n                 fill = \"#ADD0B3\") +\n  geom_vline(\n    aes(xintercept = median(math_avg)),\n    colour=\"black\", \n    linewidth = 0.8, \n    linetype = \"dashed\"\n  ) +\n  annotate(\n    geom = \"text\", \n    x = 700, \n    y = 25,\n    label = paste(\"Median =\", round(median(stu_sch$math_avg), 3)), \n    color = \"black\"\n  ) +\n  ylim(0, 30) +\n  xlim(300,900) +\n  labs(x = \"Average Scores\", y = \"Count\", title = \"Mathematics\")\nsch_math\n\n\n\n\n\n\n\n\n\nCode\nmedian_reading &lt;- round(median(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\nmean_reading &lt;- round(mean(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\nmin_reading &lt;- round(min(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\nmax_reading &lt;- round(max(stu_qqq_eda$Reading_Score, na.rm = TRUE), 0)\n\nhistogram_reading &lt;- ggplot(stu_qqq_eda, aes(x=Reading_Score)) +\n  geom_histogram(color=\"grey10\", fill='#ADD0B3', bins=30) +\n  geom_vline(aes(xintercept=median_reading), color=\"#FFFFFF\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=650, y=600, label=paste(\"Median =\", median_reading), color=\"black\", size=4) +\n  geom_vline(aes(xintercept=mean_reading), colour=\"black\", linewidth=1, linetype=\"dashed\") +\n  annotate(\"text\", x=460, y=600, label=paste(\"Mean =\", mean_reading), color=\"black\", size=4) +\n  labs(x= \"Reading Scores\", y=\"Number of Students\", title= \"Distribution of Student Reading Scores\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"))\n\nboxplot_reading &lt;- ggplot(stu_qqq_eda, aes(y=Reading_Score, x=\"\")) +\n  geom_boxplot(fill='#ADD0B3') +\n  stat_summary(fun=mean, geom=\"point\", shape=23, size=2, color=\"darkred\", fill=\"red\") +\n  geom_text(x=0.5, y=min_reading, label=min_reading, vjust=-0.5) +\n  geom_text(x=0.5, y=max_reading, label=max_reading, vjust=-0.5) +\n  coord_flip() +\n  labs(title=\"Boxplot of Reading Scores\", x=\"\", y=\"\") +\n  theme_minimal() +\n  theme(plot.background=element_rect(fill=\"#f5f5f5\", colour=\"#f5f5f5\"),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\ncombined_plot_reading &lt;- boxplot_reading + histogram_reading + plot_layout(heights = c(1, 3))\ncombined_plot_reading\n\n\n\n\n\n\n\n\n\nCode\nsch_sci &lt;- ggplot(data = stu_sch,\n       aes(x = science_avg)) +\n  geom_histogram(bins=30,\n                 color = \"grey50\",\n                 fill = \"#ADD0B3\") +\n  geom_vline(\n    aes(xintercept = median(science_avg)),\n    colour=\"black\", \n    linewidth = 0.7, \n    linetype = \"dashed\"\n  ) +\n  annotate(\n    geom = \"text\", \n    x = 700, \n    y = 25,\n    label = paste(\"Median =\", round(median(stu_sch$science_avg), 3)), \n    color = \"black\"\n  ) +\n    ylim(0, 35) +\n    xlim(300,900) +\n  labs(x = \"Average Scores\", y = \"Count\", title = \"Science\")\nsch_sci\n\n\n\n\n\n\n\n\n\n\nCode\nsch_math &lt;- ggplot(data=stu_sch,\n       aes(y = math_avg)) +\n  geom_boxplot(width=0.1, outlier.colour = \"#137a63\") +\n  stat_summary(aes(x = 0),\n               geom = \"point\",       \n               fun.y = \"mean\",\n               shape = 18,\n               colour = \"#ADD0B3\", \n               size = 3.5) +\n  xlim(c(-.1,.1))+\n  scale_x_continuous(NULL, breaks = NULL) +\n  labs(y = \"Math Average\", title = \"\")\n\nsch_read &lt;- ggplot(data=stu_sch,\n       aes(y = reading_avg)) +\n  geom_boxplot(width=0.1, outlier.colour = \"#137a63\") +\n  stat_summary(aes(x = 0),\n               geom = \"point\",       \n               fun.y = \"mean\", \n               shape = 18,\n               colour = \"#ADD0B3\", \n               size = 3.5) +\n  xlim(c(-.1,.1))+\n  scale_x_continuous(NULL, breaks = NULL) +\n  labs(y = \"Reading Average\", title = \"\")\n\nsch_sci &lt;- ggplot(data=stu_sch,\n       aes(y = science_avg)) +\n  geom_boxplot(width=0.1, outlier.colour = \"#137a63\") +\n  stat_summary(aes(x = 0),\n               geom = \"point\",       \n               fun.y = \"mean\",  \n               shape = 18,\n               colour = \"#ADD0B3\", \n               size = 3.5) +\n  xlim(c(-.1,.1)) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  labs(y = \"Science Average\", title = \"\")\n\nsch_math + sch_read + sch_sci\n\n\n\n\n\n\nCritique\n\nClarity:\n\nLack of integrated comparison, making it difficult to perform quick and direct comparisons between subjects.\nInsufficient integration of information, with independent charts leading to dispersed information, requiring the audience to shift their attention between different charts to gain a comprehensive understanding.\nUnclear identification: The box plots do not provide specific identifiers (such as School ID) like the scatter plot, which makes it less convenient to identify specific data points (such as particularly high or low school average scores).\n\nAesthetics：\n\nVisual effects: The scatter plot provides visual distinction through color coding, which helps to quickly identify the distribution of different subjects.\n\n\n\n\nRemake\n\n\nCode\n# Scatter plot for Avg Math Score vs School ID\np1 &lt;- ggplot(stu_sch, aes(x = School_ID, y = math_avg)) +\n  geom_point(aes(color = School_ID), alpha = 0.6) +\n  ggtitle(\"Scatter Plot of Avg Math Scores by School ID\") +\n  xlab(\"School ID\") +\n  ylab(\"Average Math Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Scatter plot for Avg Reading Score vs School ID\np2 &lt;- ggplot(stu_sch, aes(x = School_ID, y = reading_avg)) +\n  geom_point(aes(color = School_ID), alpha = 0.6) +\n  ggtitle(\"Scatter Plot of Avg Reading Scores by School ID\") +\n  xlab(\"School ID\") +\n  ylab(\"Average Reading Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Scatter plot for Avg Science Score vs School ID\np3 &lt;- ggplot(stu_sch, aes(x = School_ID, y = science_avg)) +\n  geom_point(aes(color = School_ID), alpha = 0.6) +\n  ggtitle(\"Scatter Plot of Avg Science Scores by School ID\") +\n  xlab(\"School ID\") +\n  ylab(\"Average Science Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plotting all three scatter plots in a grid for comparison\ngrid.arrange(p1, p2, p3, ncol = 1)\n\n\n\n\n\n\n\nCode\nlong_data &lt;- stu_sch %&gt;%\n  gather(key = \"Subject\", value = \"Score\", math_avg, reading_avg, science_avg) %&gt;%\n  group_by(School_ID, Subject) %&gt;%\n  summarise(AverageScore = mean(Score, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Label = ifelse(AverageScore &gt; 650 | AverageScore &lt; 450, as.character(School_ID), \"\"))\n\n# Create a dot plot to compare the average performance of different schools\nggplot(long_data, aes(x = School_ID, y = AverageScore, color = Subject)) +\n  geom_point(position = position_dodge(width = 0.5)) +\n  geom_text(aes(label = Label), check_overlap = TRUE, vjust = -1, position = position_dodge(width = 0.5)) +\n  theme_minimal() +\n  labs(title = \"Comparison of School Performance in Subjects\",\n       x = \"School ID\", y = \"Average Score\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  scale_x_discrete(breaks = NULL) \n\n\n\n\n\n\n\n🔍Improvement\n\nThe scatter plot displays the average scores of all subjects for all schools within the same view, facilitating horizontal comparison and trend identification.\nAll data points are represented within a single coordinate system, making the information more centralized and space utilization more efficient.\nThe scatter plot provides a visual distinction through color coding, aiding in the quick identification of the distribution of different subjects."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-3differences-in-student-performance-among-genders",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-3differences-in-student-performance-among-genders",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "Visualization 3:Differences in Student Performance Among Genders",
    "text": "Visualization 3:Differences in Student Performance Among Genders\nThis one explores variations in academic performance between male and female students, aiming to identify any notable differences in their scores across different subjects.\n\n\nCode\ngender_math &lt;- ggplot(data = stu_qqq_eda,\n       aes(x = Gender,\n           y = Math_Score)) +\n  geom_boxplot(color=\"grey50\",\n               fill=\"#ADD0B3\") +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"#137a63\",          \n             size=3) +\n  ylim(0,1000) +\n  labs(x = \"\", y = \"Score\", title = \"Mathematics\", axis.title.y = element_blank())\n\ngender_read &lt;- ggplot(data = stu_qqq_eda,\n       aes(x = Gender,\n           y = Reading_Score)) +\n  geom_boxplot(color=\"grey50\",\n               fill=\"#ADD0B3\") +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"#137a63\",          \n             size=3) +\n  ylim(0,1000) +\n  labs(x = \"\", title = \"Reading\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank())\n\ngender_sci &lt;- ggplot(data = stu_qqq_eda,\n       aes(x = Gender,\n           y = Science_Score)) +\n  geom_boxplot(color=\"grey50\",\n               fill=\"#ADD0B3\") +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"#137a63\",          \n             size=3) +\n  ylim(0,1000) +\n  labs(x = \"\", title = \"Science\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank())\n\ngender_math + gender_read + gender_sci\n\n\n\n\n\n\nCritique\n\nClarity：\n\nData Overlap Issue: Here, using boxplots, individual points may overlap, making it difficult to distinguish between data points. Moreover, it does not clearly differentiate between the comparisons of males and females.\nLack of Data Density Information: Boxplots do not show the density of the data, that is, the concentration of data in different regions.\n\nAesthetics:\n\nThe presentation is not visually appealing and does not provide an intuitive sense of comparison. In this case, switching to density plots would be better.\n\n\n\n\nRemake\n\n\nCode\ncombined &lt;- data.frame(\n  Avg_Math = stu_qqq_eda$Math_Score, \n  Avg_Reading = stu_qqq_eda$Reading_Score, \n  Avg_Science = stu_qqq_eda$Science_Score, \n  Gender = stu_qqq_eda$Gender\n)\n# Assuming 'combined' is your data frame with the scores and gender\ncombined$Gender &lt;- factor(combined$Gender, levels = c(\"Female\", \"Male\"), labels = c(\"Female\", \"Male\"))\n\n# Density plot for Math Scores\np1 &lt;- ggplot(combined, aes(x = Avg_Math, fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Density Plot of Avg Math Scores by Gender\") +\n  xlab(\"Average Math Score\") +\n  ylab(\"Density\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Female\" = \"#e69f9a\", \"Male\" = \"#7bc8ed\"))\n\n# Density plot for Reading Scores\np2 &lt;- ggplot(combined, aes(x = Avg_Reading, fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Density Plot of Avg Reading Scores by Gender\") +\n  xlab(\"Average Reading Score\") +\n  ylab(\"Density\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Female\" = \"#e69f9a\", \"Male\" = \"#7bc8ed\"))\n\n# Density plot for Science Scores\np3 &lt;- ggplot(combined, aes(x = Avg_Science, fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Density Plot of Avg Science Scores by Gender\") +\n  xlab(\"Average Science Score\") +\n  ylab(\"Density\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Female\" = \"#e69f9a\", \"Male\" = \"#7bc8ed\"))\n\n# Arrange the plots into one composite image\ngrid.arrange(p1, p2, p3, ncol = 1)\n\n\n\n\n\n\n\n🔍Improvement\n\nVisual Appeal: Density plots with different colors are usually more visually appealing. Data\nDensity Representation: Density plots provide a visual representation of data density, which helps in understanding the distribution of data across different areas.\nImproved Understanding of Data Distribution: Density plots can better display the shape of data distribution, especially for skewed distributions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-4socioeconomic-status",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-4socioeconomic-status",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "Visualization 4:Socioeconomic Status",
    "text": "Visualization 4:Socioeconomic Status\nThe code below shows a brief summary of the dataset for socioeconomic status.\n\nsummary(stu_qqq_eda$Socioeconomic_Stat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-3.5488 -0.2327  0.4817  0.2904  0.9036  3.2780      47 \n\n\nSince there are results that show NA values, this is removed during data wrangling. The below code chunk shows the summary after the deletion of unnecessary data.\n\nsummary(stu_qqq_eda$Socioeconomic_Stat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-3.5488 -0.2327  0.4817  0.2904  0.9036  3.2780      47 \n\n\nThe distribution of the socioeconomic standings can be seen from the histogram below.\n\n\nCode\nggplot(data = stu_qqq_socio,\n       aes(x = Socioeconomic_Stat)) +\n  geom_histogram(color = \"grey50\", fill = \"#ADD0B3\", bins = 30) +\n  geom_vline(aes(xintercept = mean(Socioeconomic_Stat)), color = \"#137a63\", linetype = \"dashed\", size = 1) +\n  labs(x = \"Social Economic Standing\", y = \"Frequency\", title = \"Distribution of Socioeconomic Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\ncor1 &lt;- round(cor(stu_qqq_socio$Math_Score, stu_qqq_socio$Socioeconomic_Stat),2)\ncor2 &lt;- round(cor(stu_qqq_socio$Reading_Score, stu_qqq_socio$Socioeconomic_Stat),2)\ncor3 &lt;- round(cor(stu_qqq_socio$Science_Score, stu_qqq_socio$Socioeconomic_Stat),2)\n\np1 &lt;- ggplot(data = stu_qqq_socio,\n             aes(y = Math_Score, x = Socioeconomic_Stat)) +\n  geom_point(size = 0.1, color = \"#137a63\") +\n  geom_smooth(method = lm, color = \"black\") +\n  annotate(\"text\", x = -2, y = 800, label = paste0(\"r = \", cor1), color = 'black')\n\np2 &lt;- ggplot(data = stu_qqq_socio,\n       aes(y = Reading_Score, x = Socioeconomic_Stat)) +\n  geom_point(size = 0.1, color = \"#137a63\")+\n  geom_smooth(method = lm, color = \"black\") + \n  annotate(\"text\", x = -2, y = 800, label = paste0(\"r = \", cor2), color = 'black')\n\np3 &lt;- ggplot(data = stu_qqq_socio,\n       aes(y = Science_Score, x = Socioeconomic_Stat)) +\n  geom_point(size = 0.1, color = \"#137a63\")+\n  geom_smooth(method = lm, color = \"black\") +\n  annotate(\"text\", x = -2, y = 800, label = paste0(\"r = \", cor3), color = 'black')\n\np1/p2/p3\n\n\n\n\n\n\nCritique\n\nClarity：\n\nI think both charts are quite clear, but for the first one, “Distribution of Socioeconomic Status,” I believe it could be further improved by adding annotations for the mean and median, which would likely enhance its clarity.\n\nAesthetics:\n\nFor the second chart, I suggest representing the three subjects with different colors to increase color diversity and to better differentiate between the subjects, as the current design might cause some confusion.\n\n\n\n\nRemake\n\n\nCode\nmean_value &lt;- mean(stu_qqq_socio$Socioeconomic_Stat, na.rm = TRUE)\nmedian_value &lt;- median(stu_qqq_socio$Socioeconomic_Stat, na.rm = TRUE)\n\nggplot(data = stu_qqq_socio, aes(x = Socioeconomic_Stat)) +\n  geom_histogram(color = \"grey50\", fill = \"#ADD0B3\", bins = 30) +\n  geom_vline(xintercept = mean_value, color = \"#137a63\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = median_value, color = \"blue\", linetype = \"dotted\", size = 1) +\n  geom_text(aes(x = mean_value, y = 800, label = paste(\"Mean:\", round(mean_value, 2))), color = \"#137a63\", vjust = -0.5, hjust = 1.5) +\n  geom_text(aes(x = median_value, y = 800, label = paste(\"Median:\", round(median_value, 2))), color = \"blue\", vjust = -0.5, hjust = -0.5) +\n  labs(x = \"Social Economic Standing\", y = \"Frequency\", title = \"Distribution of Socioeconomic Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\ncor1 &lt;- round(cor(stu_qqq_socio$Math_Score, stu_qqq_socio$Socioeconomic_Stat), 2)\ncor2 &lt;- round(cor(stu_qqq_socio$Reading_Score, stu_qqq_socio$Socioeconomic_Stat), 2)\ncor3 &lt;- round(cor(stu_qqq_socio$Science_Score, stu_qqq_socio$Socioeconomic_Stat), 2)\n\ncolor_math &lt;- \"#89A0B0\"      # 淡蓝色\ncolor_reading &lt;- \"#A0B089\"   # 淡绿色\ncolor_science &lt;- \"#B089A0\"   # 淡紫色\n\np1 &lt;- ggplot(data = stu_qqq_socio, aes(y = Math_Score, x = Socioeconomic_Stat)) +\n  geom_point(size = 0.1, color = color_math) +\n  geom_smooth(method = lm, color = \"black\") +\n  annotate(\"text\", x = -2, y = 800, label = paste0(\"r = \", cor1), color = 'black') +\n  labs(title = \"Math Score vs Socioeconomic Status\")\n\np2 &lt;- ggplot(data = stu_qqq_socio, aes(y = Reading_Score, x = Socioeconomic_Stat)) +\n  geom_point(size = 0.1, color = color_reading) +\n  geom_smooth(method = lm, color = \"black\") + \n  annotate(\"text\", x = -2, y = 800, label = paste0(\"r = \", cor2), color = 'black') +\n  labs(title = \"Reading Score vs Socioeconomic Status\")\n\np3 &lt;- ggplot(data = stu_qqq_socio, aes(y = Science_Score, x = Socioeconomic_Stat)) +\n  geom_point(size = 0.1, color = color_science) +\n  geom_smooth(method = lm, color = \"black\") +\n  annotate(\"text\", x = -2, y = 800, label = paste0(\"r = \", cor3), color = 'black') +\n  labs(title = \"Science Score vs Socioeconomic Status\")\n\ngrid.arrange(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\n🔍Improvement\n\nIn the first chart, detailed annotations have been added, including the calculation of specific mean and median values.\nIn the second chart, three different colors were used to distinguish the three subjects, making it visually more appealing and clear.\ncolor_math &lt;- “#89A0B0”\ncolor_reading &lt;- “#A0B089”\ncolor_science &lt;- “#B089A0”"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#learning-points",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#learning-points",
    "title": "Take-home Exercise 2 ：DataVis Makeover",
    "section": "Learning Points",
    "text": "Learning Points\n\nThe take-home exercise 1 done by my classmates are all excellent. Optimizing my peers’ homework has allowed me to learn a lot. On one hand, it involves learning unfamiliar syntax, and on the other hand, it enables me to further apply ggplot2, ggplot2 extensions, and tidyverse packages flexibly.\nIn terms of clarity, we need to choose just the right type of graphic that ensures the statistical charts comprehensively and clearly convey all the information of the data. Moreover, it’s crucial to use efficient graphical expression to integrate and distill the key points of the information.\nAs for aesthetics, the matching of colors and the thickness of lines can affect the beauty of the image, which has trained my aesthetic skills. And it allows me to understand the data more deeply through the matching of colors."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7 : Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "title": "Hands-on Exercise 7 : Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "title": "Hands-on Exercise 7 : Choropleth Mapping with R",
    "section": "Getting Started",
    "text": "Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data-into-r",
    "title": "Hands-on Exercise 7 : Choropleth Mapping with R",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\nImporting Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/zhangshujie/Desktop/SMU/Term 2/Visual/suzyzsj/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nImporting Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\nData Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\nData wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\nJoining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 7 : Choropleth Mapping with R",
    "section": "Choropleth Mapping Geospatial Data Using tmap",
    "text": "Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\nPlotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons。\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\nPlotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nPlotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nColour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\nUsing ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\nMap Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\nMap Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMap style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\nCartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\nDrawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nBy assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\nBy defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nBy creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\nMappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#reference",
    "title": "Hands-on Exercise 7 : Choropleth Mapping with R",
    "section": "Reference",
    "text": "Reference\n\nAll about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\nGeospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\nData wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html",
    "title": "Hands-on Exercise 7a : Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#overview",
    "title": "Hands-on Exercise 7a : Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#getting-started",
    "title": "Hands-on Exercise 7a : Choropleth Mapping with R",
    "section": "Getting Started",
    "text": "Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#importing-data-into-r",
    "title": "Hands-on Exercise 7a : Choropleth Mapping with R",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\nImporting Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/zhangshujie/Desktop/SMU/Term 2/Visual/suzyzsj/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nImporting Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\nData Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\nData wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\nJoining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 7a : Choropleth Mapping with R",
    "section": "Choropleth Mapping Geospatial Data Using tmap",
    "text": "Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\nPlotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons。\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\nPlotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nPlotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nColour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\nUsing ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\nMap Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\nMap Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMap style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\nCartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\nDrawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nBy assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\nBy defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nBy creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\nMappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#reference",
    "title": "Hands-on Exercise 7a : Choropleth Mapping with R",
    "section": "Reference",
    "text": "Reference\n\nAll about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\nGeospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\nData wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class Exercise 6 :Horizon Plot",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))\n\n\n\n\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#getting-started",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#getting-started",
    "title": "In-class Exercise 6 :Horizon Plot",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))\n\n\n\n\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "",
    "text": "Proportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#the-data",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "The data",
    "text": "The data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#data-import-and-preparation",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "Creating a sf data frame from an aspatial data frame",
    "text": "Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#it-all-started-with-an-interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#it-all-started-with-an-interactive-point-symbol-map",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "It all started with an interactive point symbol map",
    "text": "It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#lets-make-it-proportional",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#lets-make-it-proportional",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "Lets make it proportional",
    "text": "Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#lets-give-it-a-different-colour",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#lets-give-it-a-different-colour",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "Lets give it a different colour",
    "text": "Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#i-have-a-twin-brothers",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#i-have-a-twin-brothers",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "I have a twin brothers :)",
    "text": "I have a twin brothers :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#all-about-tmap-package",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#all-about-tmap-package",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "All about tmap package",
    "text": "All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#geospatial-data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#geospatial-data-wrangling-1",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#data-wrangling",
    "title": "Hands-on Exercise 7b : Visualising Geospatial Point Data",
    "section": "Data wrangling",
    "text": "Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html",
    "title": "Hands-on Exercise 7c : Analytical Mapping",
    "section": "",
    "text": "pacman::p_load(tmap, tidyverse, sf)\n\n\n\n\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#getting-started",
    "title": "Hands-on Exercise 7c : Analytical Mapping",
    "section": "",
    "text": "pacman::p_load(tmap, tidyverse, sf)\n\n\n\n\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 7c : Analytical Mapping",
    "section": "Basic Choropleth Mapping",
    "text": "Basic Choropleth Mapping\n\nVisualising distribution of non-functional water point\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 7c : Analytical Mapping",
    "section": "Choropleth Map for Rates",
    "text": "Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\nDeriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\nPlotting map of rate\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#extreme-value-maps",
    "title": "Hands-on Exercise 7c : Analytical Mapping",
    "section": "Extreme Value Maps",
    "text": "Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\nPercentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\nData Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values.\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\nWhy writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\nCreating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\nA percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\nTest drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\nBox map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\nCreating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\nCreating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\nTest drive the newly created function\nLet’s test the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\nBoxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#loading-r-packages",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "2.1 Loading R Packages",
    "text": "2.1 Loading R Packages\nIn this take-home exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data\ndplyr for wrangling data\nsf for handling geospatial data.\ntmap for Creating thematic maps.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(ggplot2,readr,readxl, dplyr,lubridate,reshape2,pheatmap, tidyverse,sf,viridis,terra,gstat,tmap,tibble,leaflet)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#importing-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#importing-data",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\nWe obtained Singapore weather data for the years 2014 to 2023 from the Meteorological Service Singaporewebsite, along with latitude and longitude data records for different sites. We will proceed to import this data.\n\ndata &lt;- read.csv(\"data/weather.csv\")\nstation_records &lt;- read_excel(\"data/station_records.xlsx\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#summary-statistics-of-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#summary-statistics-of-data",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "2.3 Summary Statistics of Data",
    "text": "2.3 Summary Statistics of Data\nFirst, let’s take a look at the basic structure of the data.\n\nsummary(data)\n\n   Station               Year          Month             Day       \n Length:204464      Min.   :2014   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:2016   1st Qu.: 3.000   1st Qu.: 8.00  \n Mode  :character   Median :2018   Median : 6.000   Median :16.00  \n                    Mean   :2018   Mean   : 6.457   Mean   :15.73  \n                    3rd Qu.:2021   3rd Qu.: 9.000   3rd Qu.:23.00  \n                    Max.   :2024   Max.   :12.000   Max.   :31.00  \n                    NA's   :425    NA's   :425      NA's   :425    \n Daily.Rainfall.Total..mm. Highest.30.min.Rainfall..mm.\n Length:204464             Length:204464               \n Class :character          Class :character            \n Mode  :character          Mode  :character            \n                                                       \n                                                       \n                                                       \n                                                       \n Highest.60.min.Rainfall..mm. Highest.120.min.Rainfall..mm.\n Length:204464                Length:204464                \n Class :character             Class :character             \n Mode  :character             Mode  :character             \n                                                           \n                                                           \n                                                           \n                                                           \n Mean.Temperature...C. Maximum.Temperature...C. Minimum.Temperature...C.\n Length:204464         Length:204464            Length:204464           \n Class :character      Class :character         Class :character        \n Mode  :character      Mode  :character         Mode  :character        \n                                                                        \n                                                                        \n                                                                        \n                                                                        \n Mean.Wind.Speed..km.h. Max.Wind.Speed..km.h.\n Length:204464          Length:204464        \n Class :character       Class :character     \n Mode  :character       Mode  :character     \n                                             \n                                             \n                                             \n                                             \n\n\n\nglimpse(data)\n\nRows: 204,464\nColumns: 13\n$ Station                       &lt;chr&gt; \"Paya Lebar\", \"Paya Lebar\", \"Paya Lebar\"…\n$ Year                          &lt;int&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ Month                         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Day                           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ Daily.Rainfall.Total..mm.     &lt;chr&gt; \"0\", \"0\", \"2.2\", \"0.6\", \"10.5\", \"31.2\", …\n$ Highest.30.min.Rainfall..mm.  &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Highest.60.min.Rainfall..mm.  &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Highest.120.min.Rainfall..mm. &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Mean.Temperature...C.         &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Maximum.Temperature...C.      &lt;chr&gt; \"29.5\", \"31.7\", \"31.1\", \"32.3\", \"27\", \"2…\n$ Minimum.Temperature...C.      &lt;chr&gt; \"24.8\", \"25\", \"25.1\", \"23.7\", \"23.8\", \"2…\n$ Mean.Wind.Speed..km.h.        &lt;chr&gt; \"15.8\", \"16.5\", \"14.9\", \"8.9\", \"11.9\", \"…\n$ Max.Wind.Speed..km.h.         &lt;chr&gt; \"35.3\", \"37.1\", \"33.5\", \"35.3\", \"33.5\", …\n\n\nFrom the basic overview of the data, we can see that there are 204,464 rows and 13 columns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "",
    "text": "In the research project “Be Weatherwise or Otherwise,” we aim to visualize the weather conditions of the past decade, from 2014 to 2023, with clarity. This will assist future residents in making informed decisions when purchasing homes. Despite Singapore’s relatively small geographic size, it experiences varied weather patterns across different regions.\nFor this take-home exercise, I will be responsible for the temperature part and accomplish the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nI will utilize suitable R packages for data visualization. Additionally, I will provide a detailed discussion and explanation of:\n\nthe data preparation process,\nthe selection of data visualisation techniques used,\nand the data visualisation design and interactivity principles and best practices implemented."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#cleanse-missing-values",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#cleanse-missing-values",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "2.4 Cleanse missing values",
    "text": "2.4 Cleanse missing values\nAdditionally, there is a significant presence of missing values indicated by the “?” and “�” characters within the data. This suggests that data cleaning will be necessary as part of our subsequent analysis.\n\ndata &lt;- read_csv(\"data/weather.csv\", na = c(\"?\", \"�\"))\n\ndata &lt;- data %&gt;%\n  dplyr::filter(Year &gt;= 2014, Year &lt;= 2023)\n\ncolnames(data) &lt;- c(\n  'Station', 'Year', 'Month', 'Day', 'DailyRainfall',\n  'Highest30minRainfall', 'Highest60minRainfall', 'Highest120minRainfall',\n  'MeanTemperature', 'MaxTemperature', 'MinTemperature',\n  'MeanWindSpeed', 'MaxWindSpeed'\n)\n\ndata &lt;- data %&gt;%\n  mutate(\n    DailyRainfall = as.numeric(DailyRainfall),\n    Highest30minRainfall = as.numeric(Highest30minRainfall),\n    Highest60minRainfall = as.numeric(Highest60minRainfall),\n    Highest120minRainfall = as.numeric(Highest120minRainfall),\n    MeanTemperature = as.numeric(MeanTemperature),\n    MaxTemperature = as.numeric(MaxTemperature),\n    MinTemperature = as.numeric(MinTemperature)\n  ) %&gt;%\n  \n  suppressWarnings()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#handling-missing-values",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#handling-missing-values",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "2.5 Handling missing values",
    "text": "2.5 Handling missing values\nDue to the excessive number of missing values in temperature data, I will adopt a certain strategy. If MeanTemperature is available, it will be retained. However, if it is missing, I will use the average of MaxTemperature and MinTemperature as the daily mean temperature. All temperatures will then be recorded as CalculatedMeanTemp.\nAdditionally, I will compute the monthly average data (monthly_avg_temp) for future using.\n\n\nCode\n# Prepare the data: calculate mean temperature if it's not provided\ndata &lt;- data %&gt;%\n  mutate(CalculatedMeanTemp = ifelse(is.na(MeanTemperature),\n                                     (MaxTemperature + MinTemperature) / 2,\n                                     MeanTemperature))\n\n# Calculate monthly average temperature for each station\nmonthly_avg_temp &lt;- data %&gt;%\n  group_by(Station, Year, Month) %&gt;%\n  summarise(MonthlyAvgTemp = mean(CalculatedMeanTemp, na.rm = TRUE), .groups = 'drop')\n\nmonthly_avg_temp\n\n\n# A tibble: 6,657 × 4\n   Station    Year Month MonthlyAvgTemp\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1 Admiralty  2014     1           25.8\n 2 Admiralty  2014     2           26.8\n 3 Admiralty  2014     3           27.4\n 4 Admiralty  2014     4           27.8\n 5 Admiralty  2014     5           28.2\n 6 Admiralty  2014     6           28.9\n 7 Admiralty  2014     7           28.6\n 8 Admiralty  2014     8           27.7\n 9 Admiralty  2014    10           28.1\n10 Admiralty  2014    11           27.2\n# ℹ 6,647 more rows\n\n\nAfter implementing the above strategy, I will recalculate the total number of missing values for each site.\n\n\nCode\n# Calculate the total number of missing values per station\nmissing_values_by_station &lt;- monthly_avg_temp %&gt;%\n  group_by(Station) %&gt;%\n  summarise(TotalMissing = sum(is.na(MonthlyAvgTemp), na.rm = TRUE)) %&gt;%\n  arrange(desc(TotalMissing))\n\nmissing_values_by_station\n\n\n# A tibble: 63 × 2\n   Station                 TotalMissing\n   &lt;chr&gt;                          &lt;int&gt;\n 1 Botanic Garden                   120\n 2 Buona Vista                      120\n 3 Choa Chu Kang (Central)          120\n 4 Jurong Pier                      120\n 5 Kent Ridge                       120\n 6 Macritchie Reservoir             120\n 7 Mandai                           120\n 8 Nicoll Highway                   120\n 9 Punggol                          120\n10 Queenstown                       120\n# ℹ 53 more rows\n\n\nI will record stations with more than 30 missing values as “stations_with_high_missing”.\n\n\nCode\nstations_with_high_missing &lt;- missing_values_by_station %&gt;%\n  filter(TotalMissing &gt; 30)\n\nstations_with_high_missing\n\n\n# A tibble: 41 × 2\n   Station                 TotalMissing\n   &lt;chr&gt;                          &lt;int&gt;\n 1 Botanic Garden                   120\n 2 Buona Vista                      120\n 3 Choa Chu Kang (Central)          120\n 4 Jurong Pier                      120\n 5 Kent Ridge                       120\n 6 Macritchie Reservoir             120\n 7 Mandai                           120\n 8 Nicoll Highway                   120\n 9 Punggol                          120\n10 Queenstown                       120\n# ℹ 31 more rows\n\n\nI will remove stations with TotalMissing greater than 30 from the monthly average data, as they will no longer be included in our project going forward.\n\n\nCode\n# On a monthly basis\n# Remove stations with TotalMissing greater than 30 from the monthly average data\nmonthly_data_filtered &lt;- monthly_avg_temp %&gt;%\n  anti_join(stations_with_high_missing, by = \"Station\")\n\nmonthly_data_filtered \n\n\n# A tibble: 2,299 × 4\n   Station    Year Month MonthlyAvgTemp\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1 Admiralty  2014     1           25.8\n 2 Admiralty  2014     2           26.8\n 3 Admiralty  2014     3           27.4\n 4 Admiralty  2014     4           27.8\n 5 Admiralty  2014     5           28.2\n 6 Admiralty  2014     6           28.9\n 7 Admiralty  2014     7           28.6\n 8 Admiralty  2014     8           27.7\n 9 Admiralty  2014    10           28.1\n10 Admiralty  2014    11           27.2\n# ℹ 2,289 more rows\n\n\nI will also remove stations with TotalMissing greater than 30 from the original data (data) and save it as data_with_mean_temp for future reference.\n\n\nCode\n# By the day\n# Remove stations with TotalMissing greater than 30 from the raw data\ndata_filtered &lt;- data %&gt;%\n  anti_join(stations_with_high_missing, by = \"Station\")\n\ndata_with_mean_temp &lt;- data_filtered %&gt;%\n  mutate(CalculatedMeanTemp = ifelse(is.na(MeanTemperature),\n                                  (MaxTemperature + MinTemperature) / 2,\n                                  MeanTemperature))\n\n\ndata_with_mean_temp\n\n\n# A tibble: 69,965 × 14\n   Station     Year Month   Day DailyRainfall Highest30minRainfall\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n 1 Paya Lebar  2014     1     1           0                     NA\n 2 Paya Lebar  2014     1     2           0                     NA\n 3 Paya Lebar  2014     1     3           2.2                   NA\n 4 Paya Lebar  2014     1     4           0.6                   NA\n 5 Paya Lebar  2014     1     5          10.5                   NA\n 6 Paya Lebar  2014     1     6          31.2                   NA\n 7 Paya Lebar  2014     1     7           0.4                   NA\n 8 Paya Lebar  2014     1     8           0                     NA\n 9 Paya Lebar  2014     1     9           0                     NA\n10 Paya Lebar  2014     1    10           4.8                   NA\n# ℹ 69,955 more rows\n# ℹ 8 more variables: Highest60minRainfall &lt;dbl&gt;, Highest120minRainfall &lt;dbl&gt;,\n#   MeanTemperature &lt;dbl&gt;, MaxTemperature &lt;dbl&gt;, MinTemperature &lt;dbl&gt;,\n#   MeanWindSpeed &lt;chr&gt;, MaxWindSpeed &lt;chr&gt;, CalculatedMeanTemp &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#dataset-import",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#dataset-import",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "2.6 Dataset Import",
    "text": "2.6 Dataset Import\nIn the end, we have obtained two usable datasets: one is the monthly average dataset ‘monthly_data_filtered’, and the other is the daily average dataset ‘data_with_mean_temp’.\n\n\nCode\nwrite_rds(data_with_mean_temp,\n          \"data/data_with_mean_temp.rds\")\ndata_with_mean_temp &lt;- read_rds(\"data/data_with_mean_temp.rds\")\nwrite.csv(data_with_mean_temp, \"data/data_with_mean_temp.csv\")\n\n\n\n\nCode\nwrite_rds(monthly_data_filtered ,\n          \"data/monthly_data_filtered .rds\")\nmonthly_data_filtered &lt;- read_rds(\"data/monthly_data_filtered .rds\")\nwrite.csv(monthly_data_filtered, \"data/monthly_data_filtered.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#heat-map",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#heat-map",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "Heat Map",
    "text": "Heat Map\nHere, I have chosen to create a heatmap using the data for the 12 months of 2023, including data for each day.\n\n\nCode\n# Filter the data for 2023\ndata_2023 &lt;- filter(data_with_mean_temp, Year == 2023)\n\n# Calculate the average daily temperature\ndaily_mean_temp_2023 &lt;- data_2023 %&gt;%\n  group_by(Month, Day) %&gt;%\n  summarise(CalculatedMeanTemp = mean(CalculatedMeanTemp, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nwide_data &lt;- dcast(daily_mean_temp_2023, Day ~ Month, value.var = \"CalculatedMeanTemp\")\n\nlong_data &lt;- melt(wide_data, id.vars = \"Day\", variable.name = \"Month\", value.name = \"CalculatedMeanTemp\")\n\nggplot(long_data, aes(x = Month, y = Day, fill = CalculatedMeanTemp))+\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"red\", name = \"Mean Temp (°C)\") +\n  labs(title = \"2023 Daily Mean Temperature Heatmap in Singapore\", x = \"Month\", y = \"Day\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # 旋转X轴标签以便更好地显示\n\n\n\n\n\nIt can be observed that, on average, temperatures tend to be relatively low from December to March, while they peak from May to October, reaching the highest levels of the year.\n\n\nCode\ndata$Date &lt;- make_date(data$Year, data$Month, data$Day)\ndata$WeekOfMonth &lt;- ceiling(day(data$Date) / 7)\n\npivot_data &lt;- data %&gt;%\n  group_by(Year, Month, WeekOfMonth) %&gt;%\n  summarize(CalculatedMeanTemp = mean(CalculatedMeanTemp, na.rm = TRUE), .groups = 'drop') %&gt;%\n  pivot_wider(names_from = Year, values_from = CalculatedMeanTemp)\n\npivot_melted &lt;- pivot_data %&gt;%\n  pivot_longer(cols = -c(Month, WeekOfMonth), names_to = \"Year\", values_to = \"Temperature\") %&gt;%\n  mutate(Month = factor(Month, levels = 1:12, labels = month.abb)) %&gt;%\n  arrange(Month, WeekOfMonth, Year)\n\nggplot(pivot_melted, aes(x = Year, y = interaction(Month, WeekOfMonth, sep = \"-\"), fill = Temperature)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = median(pivot_melted$Temperature, na.rm = TRUE)) +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Month - Week of Month\", fill = \"Mean Temperature\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nFrom this heatmap, it can be observed that as the years progress, the temperature range gradually increases. This may also reflect an increasing likelihood of extreme weather events occurring."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "title": "SuzyZsj",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "In this in-class exercise, beside tidyverse, viridis, sf and tmap libaries, two new R packages will be used, they are:\n\nterra is a replacement of the raster package. It has a very similar, but simpler, interface, and it is faster than raster. In this hands-on exercise, it will be used to create grid (also known as raster) objects as the input and output of spatial interpolation.\ngstat, an r packages for spatial and spatio-temporal geostatistical modelling, prediction and simulation. In this in-class exercise, it will be used to perform spatial interpolation.\nautomap, an r package for performing automatic variogram modelling and kriging interpolation.\n\n\npacman::p_load(sf,terra,gstat,automap, tmap,vireidis,tidyverse)\n\n\nThe downloaded binary packages are in\n    /var/folders/0g/1d3kn40577q2dmmwvz7nk6km0000gn/T//RtmpEOQRFn/downloaded_packages"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#clustering-by-station",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#clustering-by-station",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "4.1 Clustering by Station",
    "text": "4.1 Clustering by Station\nWe will perform clustering analysis on the monthly data. To conduct the clustering analysis, I will first remove all missing values.\n\n\nCode\nclean_data &lt;- na.omit(monthly_data_filtered)\n\nclean_data\n\n\n# A tibble: 2,241 × 4\n   Station    Year Month MonthlyAvgTemp\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1 Admiralty  2014     1           25.8\n 2 Admiralty  2014     2           26.8\n 3 Admiralty  2014     3           27.4\n 4 Admiralty  2014     4           27.8\n 5 Admiralty  2014     5           28.2\n 6 Admiralty  2014     6           28.9\n 7 Admiralty  2014     7           28.6\n 8 Admiralty  2014     8           27.7\n 9 Admiralty  2014    10           28.1\n10 Admiralty  2014    11           27.2\n# ℹ 2,231 more rows\n\n\nFor the different temperatures in each region, as the temperature range differences are not very large, I will divide them into three categories and perform clustering to see which stations have similar average temperatures.\n\n\nCode\nstation_means &lt;- aggregate(MonthlyAvgTemp ~ Station, clean_data, mean)\n\n\nset.seed(123) \nk &lt;- 3\n\ncluster_result &lt;- kmeans(station_means$MonthlyAvgTemp, centers = k)\n\nstation_means$Cluster &lt;- cluster_result$cluster\n\nggplot(station_means, aes(x = Station, y = MonthlyAvgTemp, color = factor(Cluster))) +\n  geom_point() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\nFrom the graph, we can see that blue points represent stations with higher average temperatures, green points represent stations with moderate average temperatures, and red points represent stations with lower average temperatures."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#clustering-by-month",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#clustering-by-month",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "4.1 Clustering by Month",
    "text": "4.1 Clustering by Month\nIn addition to clustering by site, we can also cluster by month. We will cluster into two groups to determine which months have lower temperatures and which months have higher temperatures.\n\n\nCode\nmonthly_avg_temp &lt;- clean_data %&gt;%\n  group_by(Month) %&gt;%\n  summarize(AvgTemp = mean(MonthlyAvgTemp))\n\nset.seed(42) \nkmeans_result &lt;- kmeans(monthly_avg_temp$AvgTemp, centers = 2)\n\nmonthly_avg_temp$Cluster &lt;- as.factor(kmeans_result$cluster)\n\nggplot(monthly_avg_temp, aes(x = Month, y = AvgTemp, color = Cluster)) +\n  geom_point(size = 4) +\n  geom_line(aes(group = 1)) + \n  scale_color_manual(values = c(\"lightblue\", \"salmon\")) +\n  labs(title = \"Cluster Analysis of Months Based on Average Temperature\",\n       x = \"Month\", y = \"Average Temperature\", color = \"Cluster\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_x_continuous(breaks = 1:12)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#interactive-map-with-site-selection-for-details-viewing",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#interactive-map-with-site-selection-for-details-viewing",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "4.1 Interactive Map (with Site Selection for Details Viewing)",
    "text": "4.1 Interactive Map (with Site Selection for Details Viewing)\n\n\nCode\nmerged_data &lt;- left_join(monthly_data_filtered , station_records, by = \"Station\")\n\nmerged_data \n\n\n# A tibble: 2,299 × 6\n   Station    Year Month MonthlyAvgTemp Latitude Longitude\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Admiralty  2014     1           25.8     1.44      104.\n 2 Admiralty  2014     2           26.8     1.44      104.\n 3 Admiralty  2014     3           27.4     1.44      104.\n 4 Admiralty  2014     4           27.8     1.44      104.\n 5 Admiralty  2014     5           28.2     1.44      104.\n 6 Admiralty  2014     6           28.9     1.44      104.\n 7 Admiralty  2014     7           28.6     1.44      104.\n 8 Admiralty  2014     8           27.7     1.44      104.\n 9 Admiralty  2014    10           28.1     1.44      104.\n10 Admiralty  2014    11           27.2     1.44      104.\n# ℹ 2,289 more rows\n\n\n\n\nCode\n#geometry\nmerdata_sf &lt;- st_as_sf(merged_data ,\n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs=3414)\n\n\n\n\nCode\nmpsz2019 &lt;-st_read(dsn = \"data/geospatial\",layer =\"MPSZ-2019\") %&gt;%\n  st_transform(CRS =3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/zhangshujie/Desktop/SMU/Term 2/Visual/suzyzsj/ISSS608-VAA/Take-home_Ex/Take-home_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\nvalid_geom &lt;- st_is_valid(mpsz2019)\n\nif (any(!valid_geom)) {\n  cat(\"Invalid geometries detected:\", which(!valid_geom), \"\\n\")\n  \n  mpsz2019 &lt;- st_make_valid(mpsz2019)\n}\n\n\nInvalid geometries detected: 9 10 14 59 119 304 \n\n\nI will add hover information on the station basis, including Station, Year, Month, and Average Temperature (°C).\n\n\nCode\nlibrary(gstat)\nlibrary(sf)\nlibrary(sp)\nlibrary(raster)\n\njan_2023_data &lt;- merged_data[merged_data$Year == 2023 & merged_data$Month == 1, ]\n\njan_2023_sf &lt;- st_as_sf(jan_2023_data, coords = c(\"Longitude\", \"Latitude\"), crs = 4326, agr = \"constant\")\n\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = jan_2023_sf$MonthlyAvgTemp)\n\ntmap_mode(\"view\")\n\ntm &lt;- tm_shape(mpsz2019) +\n  tm_borders() +\n  tm_shape(jan_2023_sf) +\n  tm_dots(col = \"MonthlyAvgTemp\", palette = \"YlOrRd\", size = 0.2, \n          popup.vars = c(\"Station\" = \"Station\", \"Year\" = \"Year\", \"Month\" = \"Month\", \"Avg Temp(°C)\" = \"MonthlyAvgTemp\")) +\n  tm_layout(title = \"2023 January Average Temperature in Singapore\")\n\ntm"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#map-of-average-temperature-distribution-in-2023",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#map-of-average-temperature-distribution-in-2023",
    "title": "Take-home Exercise 4 ：Prototyping Modules for Be Weatherwise or Otherwise",
    "section": "4.2 Map of Average Temperature Distribution in 2023",
    "text": "4.2 Map of Average Temperature Distribution in 2023\nCalculate the average temperature for each station in the year 2023 (ignoring missing values). Then, remove any records where AvgTp is NA due to missing values. Finally, display this processed dataset.\n\n\nCode\ntpData1 &lt;- data_2023 %&gt;%\n  group_by(Station) %&gt;%\n  summarise(AvgTp = mean(MeanTemperature, na.rm = TRUE)) %&gt;%\n  filter(!is.na(AvgTp)) %&gt;%\n  ungroup() \nprint(tpData1)\n\n\n# A tibble: 17 × 2\n   Station               AvgTp\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Admiralty              27.8\n 2 Ang Mo Kio             28.0\n 3 Changi                 28.2\n 4 Choa Chu Kang (South)  27.9\n 5 Clementi               27.8\n 6 East Coast Parkway     28.6\n 7 Jurong (West)          27.6\n 8 Jurong Island          28.2\n 9 Newton                 27.7\n10 Pasir Panjang          28.3\n11 Paya Lebar             28.2\n12 Pulau Ubin             27.5\n13 Sembawang              27.6\n14 Sentosa Island         28.2\n15 Tai Seng               28.4\n16 Tengah                 27.5\n17 Tuas South             28.2\n\n\nFirstly, merge the ‘tpData1’ and ‘station_records’ datasets using a left join. Then, convert the merged data into a spatial data format. Finally, transform its coordinate system for further geographical spatial analysis.\n\n\nCode\ntpData1 &lt;- tpData1 %&gt;%\n  left_join(station_records)\n\ntpData_sf1 &lt;- st_as_sf(tpData1,\n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs=3414)\n\n\nContinuing with the geographic spatial data processing, this involves:\n\nCreating raster data and performing coordinate transformations,\nEstablishing geographic spatial point data, and\nFiltering these point data based on specific spatial extents.\n\n\n\nCode\ngrid &lt;- terra::rast(mpsz2019, nrows = 690, ncols = 1075)\nxy &lt;- terra::xyFromCell(grid, 1:ncell(grid))\n\nsf::sf_use_s2(FALSE)\n\ncoop &lt;- st_as_sf(as.data.frame(xy),\n                 coords = c(\"x\",\"y\"),\n                 crs = st_crs(mpsz2019))\n\ncoop &lt;- st_filter(coop,mpsz2019)\n\n\nI set up a geostatistical model for spatial interpolation and confirmed the coordinate reference system of the data to prepare for subsequent spatial analysis.\n\n\nCode\nres &lt;- gstat(formula = AvgTp ~ 1,\n             locations = tpData_sf1,\n             nmax = 15,\n             set = list(idp = 0))\n\ntpData_sf_crs1 &lt;- st_crs(tpData_sf1)\n\nprint(tpData_sf_crs1)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nEnsure that the spatial points “coop” used for prediction and the analysis model “tpData_sf1” are in the same coordinate system. Then, utilize kriging interpolation or similar spatial interpolation methods to predict the average temperature for these points. Use ‘st_transform’ for coordinate system transformation, and ‘predict’ for predicting temperature values after transforming the coordinate system.\n\n\nCode\ncoop &lt;- st_transform(coop, crs = tpData_sf_crs1)\nresp &lt;- predict(res,coop)\n\n\n[inverse distance weighted interpolation]\n\n\n\nFirst, spatial data was prepared through coordinate transformation and rasterization. Then, the tmap package was utilized to draw a raster map, displaying the predicted temperature values across space.\n\n\nCode\nresp &lt;- st_transform(resp, crs = terra::crs(grid))\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\npred &lt;- terra::rasterize(resp, grid, field = \"pred\", fun = 'mean')\n\n#print(terra::values(pred))\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) +\n  tm_raster(alpha = 0.6, palette = \"-viridis\", n = 6) \n\n\n\n\n\nFrom the graph, we can observe:\nThe color legend indicates a temperature range from approximately 27.92 to 28.04 degrees Celsius.\nWarm colors (yellow to green) represent areas with higher temperatures.\nCool colors (blue to purple) represent areas with lower temperatures.\n\nNorthern Region: The northern region appears at the top of the graph, indicating slightly lower temperatures according to the legend.\nCentral (Southern Location): The southern region appears at the bottom of the graph, displaying blue to purple hues, suggesting temperatures in the south are relatively moderate, warmer than the northern region.\nEastern Region: The eastern region is situated on the right side of the graph, with average temperatures falling within a moderate range.\nWestern Region: The western region is located on the left side of the graph, with temperatures leaning towards the higher side. The northwest tends to be slightly cooler, while temperatures in the southwest may be higher."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "In this in-class exercise, beside tidyverse, viridis, sf and tmap libaries, two new R packages will be used, they are:\n\nterra is a replacement of the raster package. It has a very similar, but simpler, interface, and it is faster than raster. In this hands-on exercise, it will be used to create grid (also known as raster) objects as the input and output of spatial interpolation.\ngstat, an r packages for spatial and spatio-temporal geostatistical modelling, prediction and simulation. In this in-class exercise, it will be used to perform spatial interpolation.\nautomap, an r package for performing automatic variogram modelling and kriging interpolation.\n\n\npacman::p_load(sf,terra,gstat,automap, tmap,vireidis,tidyverse)\n\n\nThe downloaded binary packages are in\n    /var/folders/0g/1d3kn40577q2dmmwvz7nk6km0000gn/T//RtmpEOQRFn/downloaded_packages"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#the-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#the-data",
    "title": "In-class Exercise 7",
    "section": "The Data",
    "text": "The Data\nThree data sets will be used in this exercise, they are:\n\nRainfallStation.csv provides location information of existing rainfall stations in Singapore. The data is downloaded from Meteological Service Singapore.\nDAILYDATA_202402.csv provides weather data are rainfall stations for the month February, 2024. The data is also downloaded from Meteological Service Singapore.\nMPSZ-2019 contains planning subzone boundary of URA Master Plan 2019. It is downloaded from data.gov.sg. The original data is in kml format.\n\n\nImporting rainfall station data\nIn the code chunk below, read_csv() of readr package is used to import RainfallStation.csv. rfstations, the output object is in tibble data.frame format.\n\nrfstations &lt;-read_csv(\"data/aspatial/RainfallStation.csv\")\n\n\n\nImporting rainfall record data\nIn the code chunk below, read_csv() of readr package is used to import DAILYDATA_202402.csv. rfdata, the output object is in tibble data.frame format.\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1,5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum (`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\nselect() of dplyr package is used to retain column 1 and 5 of the input data.\ngroup_by() and summarise() of dplyr package are used to compute the total monthly rainfall from Daily Rainfall Total (mm) field. The output is stored in a new field called MONTHSUM.\n\n\n\n\n\nConverting aspatial data into geospatial data\nNext, left_join() of dplyr is used to join rfstations to rfdata by using the code chunk below.\n\nrfdata &lt;- rfdata  %&gt;%\n  left_join(rfstations)\n\nIn the code chunk below, st_as_sf() of sf package is used to convert rfdata into a simple feature data.frame object called rfdata_sf.\n\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs=3414)\n\n\n\n\n\n\n\nNote\n\n\n\n\nFor coords argument, it is important to map the X (i.e. Longitude) first, then follow by the Y (i.e. Latitude).\ncrs = 4326 indicates that the source data is in wgs84 coordinates system.\nst_transform() of sf package is then used to transform the source data from wgs84 to svy21 projected coordinates system.\nsvy21 is the official projected coordinates of Singapore. 3414 is the EPSG code of svy21.\n\n\n\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial1\", \n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/zhangshujie/Desktop/SMU/Term 2/Visual/suzyzsj/ISSS608-VAA/In-class_Ex/In-class_Ex07/data/geospatial1' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe source data is in wgs84 coordinates system, hence st_tranform() of sf package is used to theo output sf data.frame into svy21 project coordinates system.\n\n\n\n\n\nVisualising the data prepared\nIt is always a good practice to visualise the data prepared. In the code chunk below, tmap functions are used to create a dot map showing locations of rainfall station in Singapore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(rfdata_sf) +\n  tm_dots(col = \"red\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nIn the code chunk below, tmap functions are used to create a quantitative dot map of rainfall distribution by rainfall station in Singaspore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019) +\n  tm_borders() +\ntm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#spatial-interpolation-gstat-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#spatial-interpolation-gstat-method",
    "title": "In-class Exercise 7",
    "section": "Spatial Interpolation: gstat method",
    "text": "Spatial Interpolation: gstat method\nIn this section, you will gain hands-on experience on performing spatial interpolation by using gstat package. In order to perform spatial interpolation by using gstat, we first need to create an object of class called gstat, using a function of the same name: gstat. A gstat object contains all necessary information to conduct spatial interpolation, namely:\n\nThe model definition\nThe calibration data\n\nBased on its arguments, the gstat function “understands” what type of interpolation model we want to use:\n\nNo variogram model → IDW\nVariogram model, no covariates → Ordinary Kriging\nVariogram model, with covariates → Universal Kriging\n\nThe complete decision tree of gstat, including several additional methods which we are not going to use, is shown in the figure below.\n\nData preparation\nTo getting start, we need create a grid data object by using rast() of terra package as shown in the cod chunk below.\n\ngrid &lt;- terra::rast(mpsz2019, \n                    nrows = 690, \n                    ncols = 1075)\ngrid\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \n\n\nNext, a list called xy will be created by using xyFromCell() of terra package.\n\nxy &lt;- terra::xyFromCell(grid, \n                        1:ncell(grid))\nhead(xy)\n\n            x        y\n[1,] 2692.528 50231.33\n[2,] 2742.509 50231.33\n[3,] 2792.489 50231.33\n[4,] 2842.469 50231.33\n[5,] 2892.450 50231.33\n[6,] 2942.430 50231.33\n\n\n\n\n\n\n\n\nNote\n\n\n\nxyFromCell() gets coordinates of the center of raster cells for a row, column, or cell number of a SpatRaster. Or get row, column, or cell numbers from coordinates or from each other.\n\n\nLastly, we will create a data frame called coop with prediction/simulation locations by using the code chunk below.\n\ncoop &lt;- st_as_sf(as.data.frame(xy), \n                 coords = c(\"x\", \"y\"),\n                 crs = st_crs(mpsz2019))\ncoop &lt;- st_filter(coop, mpsz2019)\nhead(coop)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25883.42 ymin: 50231.33 xmax: 26133.32 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\n                   geometry\n1 POINT (25883.42 50231.33)\n2  POINT (25933.4 50231.33)\n3 POINT (25983.38 50231.33)\n4 POINT (26033.36 50231.33)\n5 POINT (26083.34 50231.33)\n6 POINT (26133.32 50231.33)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#inverse-distance-weighted-idw",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#inverse-distance-weighted-idw",
    "title": "In-class Exercise 7",
    "section": "Inverse Distance Weighted (IDW)",
    "text": "Inverse Distance Weighted (IDW)\n\nThe method\nIn the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create.\nWeighting is assigned to sample points through the use of a weighting coefficient that controls how the weighting influence will drop off as the distance from new point increases. The greater the weighting coefficient, the less the effect points will have if they are far from the unknown point during the interpolation process. As the coefficient increases, the value of the unknown point approaches the value of the nearest observational point.\nIt is important to notice that the IDW interpolation method also has some disadvantages: the quality of the interpolation result can decrease, if the distribution of sample data points is uneven. Furthermore, maximum and minimum values in the interpolated surface can only occur at sample data points. This often results in small peaks and pits around the sample data points.\n\n\nWorking with gstat\nWe are going to use three parameters of the gstat function:\n\nformula: The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata: The calibration data\nmodel: The variogram model\n\nKeep in mind that we need to specify parameter names, because these three parameters are not the first three in the gstat function definition.\nFor example, to interpolate using the IDW method we create the following gstat object, specifying just the formula and data:\ng = gstat(formula = annual ~ 1, data = rainfall)\n\nres &lt;- gstat(formula = MONTHSUM ~ 1, \n             locations = rfdata_sf, \n             nmax = 5,\n             set = list(idp = 0))\n\nNow that our model is defined, we can use predict() to actually interpolate, i.e., to calculate predicted values. The predict function accepts:\n\nA raster—stars object, such as dem\nA model—gstat object, such as g\n\nThe raster serves for two purposes:\n\nSpecifying the locations where we want to make predictions (in all methods), and\nSpecifying covariate values (in Universal Kriging only).\n\n\nresp &lt;- predict(res, coop)\n\n[inverse distance weighted interpolation]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\npred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\", \n                         fun = \"mean\")\n\nNow, we will map the interpolated surface by using tmap functions as shown in the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#kriging",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#kriging",
    "title": "In-class Exercise 7",
    "section": "Kriging",
    "text": "Kriging\n\nThe method\nKriging is one of several methods that use a limited set of sampled data points to estimate the value of a variable over a continuous spatial field. An example of a value that varies across a random spatial field might be total monthly rainfall over Singapore. It differs from Inverse Distance Weighted Interpolation discussed earlier in that it uses the spatial correlation between sampled points to interpolate the values in the spatial field: the interpolation is based on the spatial arrangement of the empirical observations, rather than on a presumed model of spatial distribution. Kriging also generates estimates of the uncertainty surrounding each interpolated value.\nIn a general sense, the kriging weights are calculated such that points nearby to the location of interest are given more weight than those farther away. Clustering of points is also taken into account, so that clusters of points are weighted less heavily (in effect, they contain less information than single points). This helps to reduce bias in the predictions.\nThe kriging predictor is an “optimal linear predictor” and an exact interpolator, meaning that each interpolated value is calculated to minimize the prediction error for that point. The value that is generated from the kriging process for any actually sampled location will be equal to the observed value at this point, and all the interpolated values will be the Best Linear Unbiased Predictors (BLUPs).\nKriging will in general not be more effective than simpler methods of interpolation if there is little spatial autocorrelation among the sampled data points (that is, if the values do not co-vary in space). If there is at least moderate spatial autocorrelation, however, kriging can be a helpful method to preserve spatial variability that would be lost using a simpler method (for an example, see Auchincloss 2007, below).\nKriging can be understood as a two-step process:\n\nfirst, the spatial covariance structure of the sampled points is determined by fitting a variogram; and\nsecond, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.\n\nKriging methods require a variogram model. A variogram (sometimes called a “semivariogram”) is a visual depiction of the covariance exhibited between each pair of points in the sampled data. For each pair of points in the sampled data, the gamma-value or “semi-variance” (a measure of the half mean-squared difference between their values) is plotted against the distance, or “lag”, between them. The “experimental” variogram is the plot of observed values, while the “theoretical” or “model” variogram is the distributional model that best fits the data.\n\n\n\nWorking with gstat\nFirstly, we will calculate and examine the empirical variogram by using variogram() of gstat package. The function requires two arguments:\n\nformula, the dependent variable and the covariates (same as in gstat, see Section 12.2.1)\ndata, a point layer with the dependent variable and covariates as attributes\n\nas shown in the code chunk below.\n\nv &lt;- variogram(MONTHSUM ~ 1, \n               data = rfdata_sf)\nplot(v)\n\n\n\n\n\nWith reference to the comparison above, am empirical variogram model will be fitted by using fit.variogram() of gstat package as shown in the code chunk below.\n\nfv &lt;- fit.variogram(object = v,\n                    model = vgm(\n                      psill = 0.5, \n                      model = \"Sph\",\n                      range = 5000, \n                      nugget = 0.1))\nfv\n\n  model     psill    range\n1   Nug 0.1129190    0.000\n2   Sph 0.5292397 5213.396\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSpatial interpolation is not a rocket science, students should try to explore the method by changing psill, model, range, nugget arguments in order to understand how the final surface map will be affected by different options used.\n\n\nWe can visualise how well the observed data fit the model by plotting fv using the code chunk below.\n\nplot(v, fv)\n\n\n\n\nThe plot above reveals that the empirical model fits rather well. In view of this, we will go ahead to perform spatial interpolation by using the newly derived model as shown in the code chunk below.\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           data = rfdata_sf, \n           model = fv)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model     psill    range\nvar1[1]   Nug 0.1129190    0.000\nvar1[2]   Sph 0.5292397 5213.396\n\n\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\nresp\n\nSimple feature collection with 314019 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2692.528 ymin: 15773.73 xmax: 56371.45 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   var1.pred  var1.var                  geometry        x        y     pred\n1   131.0667 0.6608399 POINT (25883.42 50231.33) 25883.42 50231.33 131.0667\n2   130.9986 0.6610337  POINT (25933.4 50231.33) 25933.40 50231.33 130.9986\n3   130.9330 0.6612129 POINT (25983.38 50231.33) 25983.38 50231.33 130.9330\n4   130.8698 0.6613782 POINT (26033.36 50231.33) 26033.36 50231.33 130.8698\n5   130.8092 0.6615303 POINT (26083.34 50231.33) 26083.34 50231.33 130.8092\n6   130.7514 0.6616697 POINT (26133.32 50231.33) 26133.32 50231.33 130.7514\n7   130.6965 0.6617971  POINT (26183.3 50231.33) 26183.30 50231.33 130.6965\n8   130.6446 0.6619131 POINT (26233.28 50231.33) 26233.28 50231.33 130.6446\n9   130.5958 0.6620184 POINT (26283.26 50231.33) 26283.26 50231.33 130.5958\n10  132.5484 0.6542154 POINT (25033.76 50181.32) 25033.76 50181.32 132.5484\n\n\nIn order to create a raster surface data object, rasterize() of terra is used as shown in the code chunk below.\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\nkpred\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :      last \nmin value   :  72.77826 \nmax value   : 195.53284 \n\n\n\n\nMapping the interpolated rainfall raster\nFinally, tmap functions are used to map the interpolated rainfall raster (i.e. kpred) by using the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\nAutomatic variogram modelling\nBeside using gstat to perform variogram modelling manually, autofirVariogram() of automap package can be used to perform varigram modelling as shown in the code chunk below.\n\nv_auto &lt;- autofitVariogram(MONTHSUM ~ 1, \n                           rfdata_sf)\nplot(v_auto)\n\n\n\n\n\nv_auto\n\n$exp_var\n   np      dist     gamma dir.hor dir.ver   id\n1  15  1957.436  311.9613       0       0 var1\n2  33  3307.349  707.7685       0       0 var1\n3  54  4861.368  848.1314       0       0 var1\n4 116  6716.531  730.3969       0       0 var1\n5 111  9235.708 1006.5381       0       0 var1\n6 120 11730.199 1167.5988       0       0 var1\n7 135 14384.636 1533.5903       0       0 var1\n\n$var_model\n  model    psill   range kappa\n1   Nug     0.00       0   0.0\n2   Ste 24100.71 1647955   0.3\n\n$sserr\n[1] 0.2178294\n\nattr(,\"class\")\n[1] \"autofitVariogram\" \"list\"            \n\n\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           model = v_auto$var_model,\n           data = rfdata_sf)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model    psill   range kappa\nvar1[1]   Nug     0.00       0   0.0\nvar1[2]   Ste 24100.71 1647955   0.3\n\n\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/data/geospatial1/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex07/data/geospatial1/MPSZ-2019.html",
    "title": "SuzyZsj",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#the-data",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "The Data",
    "text": "The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\nThe edges data\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\nThe nodes data\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\nImporting network data from files\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nReviewing the imported data\nNext, examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\nWrangling time\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\nReviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\nWrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\nReviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating network objects using tidygraph",
    "text": "Creating network objects using tidygraph\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\nThe tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\nThe dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\nUsing tbl_graph() to build tidygraph data model.\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\nReviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\nReviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\nChanging the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "Plotting Static Network Graphs with ggraph package",
    "text": "Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\nPlotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\nChanging the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\nChanging the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\nWorking with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\n\nFruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\nModifying network nodes\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\nModifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-facet-graphs",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating facet graphs",
    "text": "Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\nWorking with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\nWorking with facet_edges()\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\nA framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\nWorking with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#network-metrics-analysis",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "Network Metrics Analysis",
    "text": "Network Metrics Analysis\n\nComputing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\nVisualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\nVisualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 9:Modelling, Visualising and Analysing Network Data with R",
    "section": "Building Interactive Network Graph with visNetwork",
    "text": "Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nCan move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nCan also zoom in and out on the plot and move it around to re-center it.\n\n\n\nData preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\nPlotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\nWorking with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\nWorking with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nWorking with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\nInteractivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  }
]